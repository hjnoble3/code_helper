{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 33 code files to process\n",
      "Previously processed: 316 files\n",
      "Files left to process: 33\n",
      "Warning: Only 33 code files found, requested 300\n",
      "Processing file 1/33: Badge.svelte\n",
      "Files left to process: 32\n",
      "Processing file 2/33: Banner.svelte\n",
      "Files left to process: 31\n",
      "Processing file 3/33: Checkbox.svelte\n",
      "Files left to process: 30\n",
      "Processing file 4/33: CodeEditor.svelte\n",
      "Files left to process: 29\n",
      "Processing file 5/33: Collapsible.svelte\n",
      "Files left to process: 28\n",
      "Processing file 6/33: ConfirmDialog.svelte\n",
      "Files left to process: 27\n",
      "Processing file 7/33: DragGhost.svelte\n",
      "Files left to process: 26\n",
      "Processing file 8/33: Drawer.svelte\n",
      "Files left to process: 25\n",
      "Processing file 9/33: Dropdown.svelte\n",
      "Files left to process: 24\n",
      "Processing file 10/33: FileItem.svelte\n",
      "Files left to process: 23\n",
      "Processing file 11/33: FileItemModal.svelte\n",
      "Files left to process: 22\n",
      "Processing file 12/33: Folder.svelte\n",
      "Files left to process: 21\n",
      "Processing file 13/33: Image.svelte\n",
      "Files left to process: 20\n",
      "Processing file 14/33: ImagePreview.svelte\n",
      "Files left to process: 19\n",
      "Processing file 15/33: Loader.svelte\n",
      "Files left to process: 18\n",
      "Processing file 16/33: Marquee.svelte\n",
      "Files left to process: 17\n",
      "Processing file 17/33: Modal.svelte\n",
      "Files left to process: 16\n",
      "Processing file 18/33: Overlay.svelte\n",
      "Files left to process: 15\n",
      "Processing file 19/33: Pagination.svelte\n",
      "Files left to process: 14\n",
      "Processing file 20/33: RichTextInput.svelte\n",
      "Files left to process: 13\n",
      "Processing file 21/33: Selector.svelte\n",
      "Files left to process: 12\n",
      "Processing file 22/33: SensitiveInput.svelte\n",
      "Files left to process: 11\n",
      "Processing file 23/33: Sidebar.svelte\n",
      "Files left to process: 10\n",
      "Processing file 24/33: SlideShow.svelte\n",
      "Files left to process: 9\n",
      "Processing file 25/33: Spinner.svelte\n",
      "Files left to process: 8\n",
      "Processing file 26/33: SVGPanZoom.svelte\n",
      "Files left to process: 7\n",
      "Processing file 27/33: Switch.svelte\n",
      "Files left to process: 6\n",
      "Processing file 28/33: Tags.svelte\n",
      "Files left to process: 5\n",
      "Processing file 29/33: Textarea.svelte\n",
      "Files left to process: 4\n",
      "Processing file 30/33: Tooltip.svelte\n",
      "Files left to process: 3\n",
      "Processing file 31/33: Valves.svelte\n",
      "Files left to process: 2\n",
      "Processing file 32/33: TagInput.svelte\n",
      "Files left to process: 1\n",
      "Processing file 33/33: TagList.svelte\n",
      "Files left to process: 0\n",
      "Processed 33 files\n",
      "Files remaining in repo: 0\n",
      "Output written to combined_code.txt_20250305_155853.txt\n",
      "Processed files logged in processed_files.log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "def is_code_file(filename):\n",
    "    \"\"\"Check if a file is a programming language file based on extension\"\"\"\n",
    "    code_extensions = {\n",
    "        '.py', '.java', '.cpp', '.c', '.h', '.js',\n",
    "        '.html', '.css', '.php', '.rb', '.go', '.rs',\n",
    "        '.ts', '.sql', '.sh', '.kt', '.swift', '.cs', '.svelte'\n",
    "    }\n",
    "    return any(filename.lower().endswith(ext) for ext in code_extensions)\n",
    "\n",
    "\n",
    "def combine_code_files(repo_path, num_files, output_file=\"combined_code.txt\", log_file=\"processed_files.log\"):\n",
    "    \"\"\"\n",
    "    Combine specified number of code files from repository into one text file\n",
    "    Keeps track of processed files to avoid duplicates on subsequent runs\n",
    "    Skips the 'code_helper' folder\n",
    "    Shows progress and remaining files to process\n",
    "    \"\"\"\n",
    "    # Load previously processed files\n",
    "    processed_files = set()\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, 'r') as log:\n",
    "            processed_files = set(line.strip() for line in log)\n",
    "\n",
    "    # Get all files from repository, skipping code_helper folder\n",
    "    all_files = []\n",
    "    for root, _, files in os.walk(repo_path):\n",
    "        # Skip if the path contains 'code_helper'\n",
    "        if 'code_helper' in root.split(os.sep):\n",
    "            continue\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            if (is_code_file(file) and\n",
    "                full_path not in processed_files and\n",
    "                    os.path.isfile(full_path)):\n",
    "                all_files.append(full_path)\n",
    "\n",
    "    # Display total files found and how many are left to process\n",
    "    total_files = len(all_files)\n",
    "    print(f\"Found {total_files} code files to process\")\n",
    "    print(f\"Previously processed: {len(processed_files)} files\")\n",
    "    print(f\"Files left to process: {total_files}\")\n",
    "\n",
    "    # Check if we have enough files\n",
    "    if total_files < num_files:\n",
    "        print(f\"Warning: Only {total_files} code files found, requested {num_files}\")\n",
    "        num_files = total_files\n",
    "\n",
    "    # Process specified number of files\n",
    "    files_processed = 0\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename = f\"{output_file}_{timestamp}.txt\"\n",
    "    with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "        with open(log_file, 'a', encoding='utf-8') as logfile:\n",
    "            for i, file_path in enumerate(all_files):\n",
    "                if files_processed >= num_files:\n",
    "                    break\n",
    "                try:\n",
    "                    # Show progress\n",
    "                    print(f\"Processing file {i+1}/{total_files}: {os.path.basename(file_path)}\")\n",
    "                    print(f\"Files left to process: {total_files - (i+1)}\")\n",
    "\n",
    "                    with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                        # Write file header\n",
    "                        relative_path = os.path.relpath(file_path, repo_path)\n",
    "                        outfile.write(f\"\\n{'='*50}\\n\")\n",
    "                        outfile.write(f\"File: {relative_path}\\n\")\n",
    "                        outfile.write(f\"{'='*50}\\n\\n\")\n",
    "                        # Write file contents\n",
    "                        outfile.write(infile.read())\n",
    "                        outfile.write(\"\\n\\n\")\n",
    "                        # Log processed file\n",
    "                        logfile.write(f\"{file_path}\\n\")\n",
    "                        files_processed += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {str(e)}\")\n",
    "\n",
    "    print(f\"Processed {files_processed} files\")\n",
    "    print(f\"Files remaining in repo: {total_files - files_processed}\")\n",
    "    print(f\"Output written to {output_filename}\")\n",
    "    print(f\"Processed files logged in {log_file}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Get user input\n",
    "\n",
    "    repo_path = r\"C:\\Users\\harold.noble\\Desktop\\open-webui - Copy\\app\\src\\lib\"  # input(\"Enter the repository path: \")\n",
    "\n",
    "    while not os.path.isdir(repo_path):\n",
    "\n",
    "        print(\"Invalid path. Please enter a valid directory path.\")\n",
    "\n",
    "        repo_path = input(\"Enter the repository path: \")\n",
    "\n",
    "\n",
    "    try:\n",
    "\n",
    "        num_files = int(300) #input(\"Enter the number of files to combine: \"))\n",
    "\n",
    "        if num_files <= 0:\n",
    "\n",
    "            raise ValueError(\"Number must be positive\")\n",
    "\n",
    "    except ValueError:\n",
    "\n",
    "        print(\"Invalid number. Please enter a positive integer.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Run the combination\n",
    "\n",
    "    combine_code_files(repo_path, num_files)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DIR = r\"C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency analysis written to dependency_analysis.txt\n",
      "Processed 289 of 289 files in 22 groups\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "REPO_DIR = r\"C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\"\n",
    "SKIP_DIR = os.path.join(REPO_DIR, r\"lib\\components\\icons\")\n",
    "OUTPUT_FILE = \"dependency_analysis.txt\"\n",
    "\n",
    "\n",
    "def extract_svelte_script(file_path):\n",
    "    \"\"\"Extract script content from a Svelte file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            script_match = re.search(r'<script[^>]*>(.*?)</script>', content, re.DOTALL)\n",
    "            if script_match:\n",
    "                return script_match.group(1).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return \"\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def normalize_import_path(import_path, file_dir):\n",
    "    \"\"\"Normalize import paths to absolute paths within the repo.\"\"\"\n",
    "    # Handle relative imports\n",
    "    if import_path.startswith('./') or import_path.startswith('../'):\n",
    "        # Resolve the path relative to the importing file\n",
    "        abs_path = os.path.normpath(os.path.join(file_dir, import_path))\n",
    "        # Convert to repo-relative path\n",
    "        return os.path.relpath(abs_path, REPO_DIR).replace('\\\\', '/')\n",
    "\n",
    "    # Handle absolute imports (from repo root)\n",
    "    if import_path.startswith('/'):\n",
    "        return import_path.lstrip('/')\n",
    "\n",
    "    # Handle package imports (likely node_modules)\n",
    "    return import_path\n",
    "\n",
    "\n",
    "def get_imports(content, file_dir):\n",
    "    \"\"\"Extract and normalize all import paths from the content.\"\"\"\n",
    "    # Match both import ... from 'path' and import 'path' patterns\n",
    "    import_patterns = [\n",
    "        r'import.*from\\s*[\\'\"](.+?)[\\'\"]',  # import X from 'path'\n",
    "        r'import\\s*[\\'\"](.+?)[\\'\"]'          # import 'path'\n",
    "    ]\n",
    "\n",
    "    normalized_imports = set()\n",
    "    for pattern in import_patterns:\n",
    "        imports = re.findall(pattern, content)\n",
    "        for imp in imports:\n",
    "            normalized_imports.add(normalize_import_path(imp, file_dir))\n",
    "\n",
    "    return normalized_imports\n",
    "\n",
    "\n",
    "def resolve_import_to_files(import_path, file_map):\n",
    "    \"\"\"Resolve an import path to actual file paths in the repo.\"\"\"\n",
    "    # Direct match\n",
    "    if import_path in file_map:\n",
    "        return [import_path]\n",
    "\n",
    "    # Try adding extensions\n",
    "    for ext in ['.ts', '.js', '.svelte']:\n",
    "        with_ext = f\"{import_path}{ext}\"\n",
    "        if with_ext in file_map:\n",
    "            return [with_ext]\n",
    "\n",
    "    # Try index files\n",
    "    for ext in ['.ts', '.js', '.svelte']:\n",
    "        index_path = f\"{import_path}/index{ext}\"\n",
    "        if index_path in file_map:\n",
    "            return [index_path]\n",
    "\n",
    "    # Return empty list if no match found\n",
    "    return []\n",
    "\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"Process a file to extract its content and imports.\"\"\"\n",
    "    file_dir = os.path.dirname(file_path)\n",
    "    rel_file_path = os.path.relpath(file_path, REPO_DIR).replace('\\\\', '/')\n",
    "\n",
    "    if file_path.endswith('.svelte'):\n",
    "        content = extract_svelte_script(file_path)\n",
    "    elif file_path.endswith(('.ts', '.js')):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "            return rel_file_path, \"\", set()\n",
    "    else:\n",
    "        return rel_file_path, \"\", set()\n",
    "\n",
    "    imports = get_imports(content, file_dir)\n",
    "    return rel_file_path, content, imports\n",
    "\n",
    "\n",
    "def build_dependency_graph(file_info):\n",
    "    \"\"\"Build a graph of which files import which other files.\"\"\"\n",
    "    import_graph = defaultdict(set)\n",
    "\n",
    "    # Create a map of normalized paths to file paths\n",
    "    file_map = {}\n",
    "    for file_path in file_info:\n",
    "        rel_path = os.path.relpath(file_path, REPO_DIR).replace('\\\\', '/')\n",
    "        file_map[rel_path] = file_path\n",
    "\n",
    "    # Build the import graph\n",
    "    for file_path, info in file_info.items():\n",
    "        rel_path = os.path.relpath(file_path, REPO_DIR).replace('\\\\', '/')\n",
    "\n",
    "        for import_path in info['imports']:\n",
    "            resolved_files = resolve_import_to_files(import_path, file_map)\n",
    "\n",
    "            for resolved in resolved_files:\n",
    "                actual_file = file_map[resolved]\n",
    "                import_graph[actual_file].add(file_path)\n",
    "\n",
    "    return import_graph, file_map\n",
    "\n",
    "\n",
    "def analyze_dependencies(output_file=OUTPUT_FILE):\n",
    "    \"\"\"Analyze dependencies and write results to file.\"\"\"\n",
    "    # Collect file info, skipping the icons directory\n",
    "    file_info = {}\n",
    "    processed_files = set()\n",
    "\n",
    "    for root, _, files in os.walk(REPO_DIR):\n",
    "        if root.startswith(SKIP_DIR):\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file.endswith(('.svelte', '.ts', '.js')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                rel_path, content, imports = process_file(file_path)\n",
    "                if content is not None:  # Include even empty content\n",
    "                    file_info[file_path] = {\n",
    "                        'content': content,\n",
    "                        'imports': imports\n",
    "                    }\n",
    "\n",
    "    # Build dependency graph\n",
    "    import_graph, file_map = build_dependency_graph(file_info)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as out:\n",
    "        group_num = 1\n",
    "\n",
    "        # Continue until all files are processed\n",
    "        while len(processed_files) < len(file_info):\n",
    "            # Find files with lowest import count that haven't been processed\n",
    "            unprocessed_files = {\n",
    "                path: info for path, info in file_info.items()\n",
    "                if path not in processed_files\n",
    "            }\n",
    "\n",
    "            if not unprocessed_files:\n",
    "                break\n",
    "\n",
    "            min_imports = min(len(info['imports']) for path, info in unprocessed_files.items())\n",
    "\n",
    "            base_modules = {\n",
    "                path: info for path, info in unprocessed_files.items()\n",
    "                if len(info['imports']) == min_imports\n",
    "            }\n",
    "\n",
    "            if not base_modules:\n",
    "                break\n",
    "\n",
    "            # Add group marker\n",
    "            out.write(f\"## GROUP {group_num} - Files with {min_imports} imports\\n\\n\")\n",
    "\n",
    "            # Process each base module\n",
    "            for base_path, info in base_modules.items():\n",
    "                # Skip if already processed (safety check)\n",
    "                if base_path in processed_files:\n",
    "                    continue\n",
    "\n",
    "                rel_base_path = os.path.relpath(base_path, REPO_DIR).replace('\\\\', '/')\n",
    "\n",
    "                # Write the file name with a header and content\n",
    "                out.write(f\"### {rel_base_path}\\n\")\n",
    "                out.write(f\"{info['content']}\\n\\n\")\n",
    "\n",
    "                # Mark this file as processed\n",
    "                processed_files.add(base_path)\n",
    "\n",
    "                # Get files that import this one\n",
    "                importing_files = list(import_graph.get(base_path, set()))\n",
    "\n",
    "                # Process importing files if they exist\n",
    "                for imp_path in importing_files:\n",
    "                    if imp_path not in processed_files:  # Only include files not yet processed\n",
    "                        rel_imp_path = os.path.relpath(imp_path, REPO_DIR).replace('\\\\', '/')\n",
    "                        out.write(f\"### {rel_imp_path}\\n\")\n",
    "                        out.write(f\"{file_info[imp_path]['content']}\\n\\n\")\n",
    "\n",
    "                        # Mark as processed\n",
    "                        processed_files.add(imp_path)\n",
    "\n",
    "            # Add separator between groups\n",
    "            out.write(f\"{'='*80}\\n\\n\")\n",
    "            group_num += 1\n",
    "\n",
    "    print(f\"Dependency analysis written to {output_file}\")\n",
    "    print(f\"Processed {len(processed_files)} of {len(file_info)} files in {group_num-1} groups\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_dependencies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
