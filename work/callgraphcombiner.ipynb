{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 289 of 289 files in 210 groups\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "REPO_DIR = r\"C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\"\n",
    "SKIP_DIR = os.path.join(REPO_DIR, r\"lib\\components\\icons\")\n",
    "OUTPUT_FILE = \"dependency_analysis.txt\"\n",
    "\n",
    "\n",
    "def normalize_import_path(import_path, file_dir):\n",
    "    \"\"\"Normalize import paths to absolute paths within the repo.\"\"\"\n",
    "    if import_path.startswith(('./', '../')):\n",
    "        abs_path = os.path.normpath(os.path.join(file_dir, import_path))\n",
    "        return os.path.relpath(abs_path, REPO_DIR).replace('\\\\', '/')\n",
    "    if import_path.startswith('/'):\n",
    "        return import_path.lstrip('/')\n",
    "    return import_path\n",
    "\n",
    "\n",
    "def get_imports(content, file_dir):\n",
    "    \"\"\"Extract and normalize all import paths from the content.\"\"\"\n",
    "    import_patterns = [\n",
    "        r'import.*from\\s*[\\'\"](.+?)[\\'\"]',\n",
    "        r'import\\s*[\\'\"](.+?)[\\'\"]'\n",
    "    ]\n",
    "    normalized_imports = set()\n",
    "    for pattern in import_patterns:\n",
    "        for imp in re.findall(pattern, content):\n",
    "            normalized_imports.add(normalize_import_path(imp, file_dir))\n",
    "    return normalized_imports\n",
    "\n",
    "\n",
    "def resolve_import_to_files(import_path, file_map):\n",
    "    \"\"\"Resolve an import path to actual file paths in the repo.\"\"\"\n",
    "    for ext in ['', '.ts', '.js', '.svelte', '/index.ts', '/index.js', '/index.svelte']:\n",
    "        path = import_path + ext if not ext.startswith('/') else import_path + ext[1:]\n",
    "        if path in file_map:\n",
    "            return [path]\n",
    "    return []\n",
    "\n",
    "\n",
    "def process_file(file_path):\n",
    "    \"\"\"Process a file to extract its full content and imports.\"\"\"\n",
    "    file_dir = os.path.dirname(file_path)\n",
    "    rel_file_path = os.path.relpath(file_path, REPO_DIR).replace('\\\\', '/')\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        imports = get_imports(content, file_dir)\n",
    "        return rel_file_path, content, imports\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return rel_file_path, \"\", set()\n",
    "\n",
    "\n",
    "def build_dependency_graph(file_info):\n",
    "    \"\"\"Build a graph of which files import which other files.\"\"\"\n",
    "    import_graph = defaultdict(set)  # file -> files that import it\n",
    "    file_map = {os.path.relpath(path, REPO_DIR).replace('\\\\', '/'): path for path in file_info}\n",
    "\n",
    "    for file_path, info in file_info.items():\n",
    "        rel_path = os.path.relpath(file_path, REPO_DIR).replace('\\\\', '/')\n",
    "        for import_path in info['imports']:\n",
    "            resolved_files = resolve_import_to_files(import_path, file_map)\n",
    "            for resolved in resolved_files:\n",
    "                if resolved in file_map:\n",
    "                    import_graph[file_map[resolved]].add(file_path)\n",
    "    return import_graph, file_map\n",
    "\n",
    "\n",
    "def analyze_dependencies(output_file=OUTPUT_FILE):\n",
    "    \"\"\"Analyze dependencies, including full file content.\"\"\"\n",
    "    # Collect file info\n",
    "    file_info = {}\n",
    "    for root, _, files in os.walk(REPO_DIR):\n",
    "        if root.startswith(SKIP_DIR):\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file.endswith(('.svelte', '.ts', '.js')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                rel_path, content, imports = process_file(file_path)\n",
    "                file_info[file_path] = {'content': content, 'imports': imports}\n",
    "\n",
    "    import_graph, file_map = build_dependency_graph(file_info)\n",
    "    processed_files = set()\n",
    "    group_num = 1\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as out:\n",
    "        while len(processed_files) < len(file_info):\n",
    "            unprocessed = {p: info for p, info in file_info.items() if p not in processed_files}\n",
    "            if not unprocessed:\n",
    "                break\n",
    "\n",
    "            # Pick one file with the fewest imports\n",
    "            base_path = min(unprocessed, key=lambda p: len(unprocessed[p]['imports']))\n",
    "            base_info = file_info[base_path]\n",
    "            rel_base_path = os.path.relpath(base_path, REPO_DIR).replace('\\\\', '/')\n",
    "\n",
    "            out.write(f\"## GROUP {group_num} - Starting with {rel_base_path}\\n\\n\")\n",
    "            out.write(f\"### {rel_base_path}\\n\")\n",
    "            out.write(f\"{base_info['content']}\\n\\n\")\n",
    "            processed_files.add(base_path)\n",
    "\n",
    "            # Include imported files\n",
    "            for import_path in base_info['imports']:\n",
    "                resolved = resolve_import_to_files(import_path, file_map)\n",
    "                for res in resolved:\n",
    "                    if res in file_map and file_map[res] not in processed_files:\n",
    "                        imp_path = file_map[res]\n",
    "                        rel_imp_path = os.path.relpath(imp_path, REPO_DIR).replace('\\\\', '/')\n",
    "                        imp_info = file_info[imp_path]\n",
    "                        out.write(f\"### {rel_imp_path}\\n\")\n",
    "                        out.write(f\"{imp_info['content']}\\n\\n\")\n",
    "                        processed_files.add(imp_path)\n",
    "\n",
    "            # Include importing files\n",
    "            importers = import_graph.get(base_path, set())\n",
    "            for imp_path in importers:\n",
    "                if imp_path not in processed_files:\n",
    "                    rel_imp_path = os.path.relpath(imp_path, REPO_DIR).replace('\\\\', '/')\n",
    "                    imp_info = file_info[imp_path]\n",
    "                    out.write(f\"### {rel_imp_path}\\n\")\n",
    "                    out.write(f\"{imp_info['content']}\\n\\n\")\n",
    "                    processed_files.add(imp_path)\n",
    "\n",
    "            out.write(f\"{'='*80}\\n\\n\")\n",
    "            group_num += 1\n",
    "\n",
    "        # Handle remaining isolated files\n",
    "        remaining = {p for p in file_info if p not in processed_files}\n",
    "        if remaining:\n",
    "            out.write(f\"## GROUP {group_num} - Remaining Isolated Files\\n\\n\")\n",
    "            for file_path in remaining:\n",
    "                rel_path = os.path.relpath(file_path, REPO_DIR).replace('\\\\', '/')\n",
    "                out.write(f\"### {rel_path}\\n\")\n",
    "                out.write(f\"{file_info[file_path]['content']}\\n\\n\")\n",
    "                processed_files.add(file_path)\n",
    "            out.write(f\"{'='*80}\\n\\n\")\n",
    "            group_num += 1\n",
    "\n",
    "    print(f\"Processed {len(processed_files)} of {len(file_info)} files in {group_num-1} groups\")\n",
    "    if len(processed_files) < len(file_info):\n",
    "        print(f\"Warning: {len(file_info) - len(processed_files)} files were not processed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
