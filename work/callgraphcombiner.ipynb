{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\routes for .svelte, .ts, and .py files...\n",
      "Generated group_unused.txt with 39 unused files\n",
      "Processed 39 of 39 files in 0 groups\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuration\n",
    "SOURCE_ROOT = r\"C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\routes\" # Your specific path\n",
    "EXTENSIONS = {'.svelte', '.ts', '.py'}\n",
    "\n",
    "def get_file_content(file_path):\n",
    "    \"\"\"Extract relevant content based on file type.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        if file_path.endswith('.svelte'):\n",
    "            match = re.search(r'<script[^>]*>(.*?)</script>', content, re.DOTALL)\n",
    "            return match.group(1).strip() if match else ''\n",
    "        return content\n",
    "\n",
    "def extract_imports(file_path, content):\n",
    "    \"\"\"Extract import paths from file content.\"\"\"\n",
    "    imports = set()\n",
    "    if file_path.endswith('.svelte') or file_path.endswith('.ts'):\n",
    "        # Match TypeScript imports\n",
    "        for match in re.finditer(r'import\\s+.*?\\s+from\\s+[\\'\"](.+?)[\\'\"]', content):\n",
    "            imp = match.group(1)\n",
    "            if imp.startswith(('./', '../')) and any(imp.endswith(ext) for ext in EXTENSIONS):\n",
    "                imports.add(imp)\n",
    "    elif file_path.endswith('.py'):\n",
    "        # Match Python imports\n",
    "        for match in re.finditer(r'from\\s+(.+?)\\s+import|import\\s+(.+?)(?:\\s+as\\s+|$)', content):\n",
    "            imp = match.group(1) or match.group(2)\n",
    "            if imp.startswith(('./', '../')):\n",
    "                imports.add(imp + '.py' if not imp.endswith('.py') else imp)\n",
    "    return imports\n",
    "\n",
    "def resolve_import_path(base_path, imp):\n",
    "    \"\"\"Resolve relative import to absolute path.\"\"\"\n",
    "    dir_name = os.path.dirname(base_path)\n",
    "    full_path = os.path.normpath(os.path.join(dir_name, imp))\n",
    "    return full_path if os.path.exists(full_path) else None\n",
    "\n",
    "def build_dependency_graph():\n",
    "    \"\"\"Build a graph of file dependencies.\"\"\"\n",
    "    file_imports = {}  # file -> set of files it imports\n",
    "    imported_by = defaultdict(set)  # file -> set of files importing it\n",
    "    file_contents = {}  # file -> content\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(SOURCE_ROOT):\n",
    "        for filename in filenames:\n",
    "            if not any(filename.endswith(ext) for ext in EXTENSIONS):\n",
    "                continue\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            relative_path = os.path.relpath(file_path, SOURCE_ROOT).replace('\\\\', '/')\n",
    "            content = get_file_content(file_path)\n",
    "            file_contents[relative_path] = content\n",
    "            imports = extract_imports(file_path, content)\n",
    "            file_imports[relative_path] = set()\n",
    "\n",
    "            for imp in imports:\n",
    "                resolved_path = resolve_import_path(file_path, imp)\n",
    "                if resolved_path:\n",
    "                    resolved_relative = os.path.relpath(resolved_path, SOURCE_ROOT).replace('\\\\', '/')\n",
    "                    file_imports[relative_path].add(resolved_relative)\n",
    "                    imported_by[resolved_relative].add(relative_path)\n",
    "\n",
    "    return file_imports, imported_by, file_contents\n",
    "\n",
    "def generate_group_files(file_imports, imported_by, file_contents):\n",
    "    \"\"\"Generate text files for each group, starting with least-used files.\"\"\"\n",
    "    # Calculate usage counts\n",
    "    usage_counts = {file: len(imported_by.get(file, set())) for file in file_contents}\n",
    "    all_files = set(file_contents.keys())\n",
    "    unused_files = {f for f in all_files if usage_counts.get(f, 0) == 0}\n",
    "\n",
    "    # Sort files by usage count (ascending) for group generation\n",
    "    sorted_files = sorted(usage_counts.items(), key=lambda x: x[1])\n",
    "\n",
    "    group_num = 1\n",
    "    processed_files = set()\n",
    "\n",
    "    # Generate groups starting with most-used files\n",
    "    for root_file, count in sorted_files:\n",
    "        if root_file in processed_files or count == 0:  # Skip unused for now\n",
    "            continue\n",
    "        importers = imported_by.get(root_file, set())\n",
    "        if not importers:\n",
    "            continue\n",
    "\n",
    "        output_file = f\"group_{group_num}.txt\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"## GROUP {group_num} - Starting with {root_file} (Imported by {count} files)\\n\\n\")\n",
    "            for importer in sorted(importers):\n",
    "                f.write(f\"### {importer}\\n\")\n",
    "                if importer.endswith('.svelte'):\n",
    "                    f.write(\"<script lang=\\\"ts\\\">\\n\")\n",
    "                    f.write(f\"{file_contents[importer]}\\n\")\n",
    "                    f.write(\"</script>\\n\\n\")\n",
    "                else:\n",
    "                    f.write(f\"{file_contents[importer]}\\n\\n\")\n",
    "                processed_files.add(importer)\n",
    "            processed_files.add(root_file)\n",
    "        print(f\"Generated {output_file}\")\n",
    "        group_num += 1\n",
    "\n",
    "    # Handle unused files in a separate file\n",
    "    if unused_files:\n",
    "        with open(\"group_unused.txt\", 'w', encoding='utf-8') as f:\n",
    "            f.write(\"## GROUP - Unused Files (Imported by 0 files)\\n\\n\")\n",
    "            for file in sorted(unused_files - processed_files):\n",
    "                f.write(f\"### {file}\\n\")\n",
    "                if file.endswith('.svelte'):\n",
    "                    f.write(\"<script lang=\\\"ts\\\">\\n\")\n",
    "                    f.write(f\"{file_contents[file]}\\n\")\n",
    "                    f.write(\"</script>\\n\\n\")\n",
    "                else:\n",
    "                    f.write(f\"{file_contents[file]}\\n\\n\")\n",
    "                processed_files.add(file)\n",
    "        print(f\"Generated group_unused.txt with {len(unused_files)} unused files\")\n",
    "\n",
    "    print(f\"Processed {len(processed_files)} of {len(file_contents)} files in {group_num-1} groups\")\n",
    "\n",
    "# Main execution\n",
    "print(f\"Scanning {SOURCE_ROOT} for .svelte, .ts, and .py files...\")\n",
    "file_imports, imported_by, file_contents = build_dependency_graph()\n",
    "generate_group_files(file_imports, imported_by, file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
