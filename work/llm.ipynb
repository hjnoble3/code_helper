{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 144 unprocessed files. Processing 10 this run.\n",
      "Skipping folders: {'ollama-0', '.github', 'code_helper'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring Files:   0%|          | 0/10 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/10: C:\\Users\\harold.noble\\Desktop\\open-webui\\app\\backend\\webui\\config.py\n",
      "Script C:\\Users\\harold.noble\\Desktop\\open-webui\\app\\backend\\webui\\config.py exceeds 7800 tokens (18515). Splitting into chunks...\n",
      "Processing chunk 1/26 of C:\\Users\\harold.noble\\Desktop\\open-webui\\app\\backend\\webui\\config.py\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script to refactor Python files one at a time, up to a limit per run, by sending them to an LLM for optimization.\n",
    "Handles scripts longer than LLM_MAX_TOKENS by splitting them into chunks.\n",
    "Maintains a persistent function/variable name mapping and applies it across all scripts.\n",
    "Tracks processed files and saves refactored code back to the original files.\n",
    "Allows skipping specified subfolders.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from typing import Dict, Tuple, Set, List\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import chardet\n",
    "\n",
    "# Constants\n",
    "DEFAULT_DIRECTORY = r\"C:\\Users\\harold.noble\\Desktop\\open-webui\\app\"\n",
    "SKIP_FOLDERS = {\".github\", \"code_helper\", \"ollama-0\"}  # Folders to skip during processing\n",
    "NAME_MAPPING_FILE = \"function_variable_mapping.txt\"\n",
    "PROCESSED_FILES_TRACKER = \"processed_files.txt\"\n",
    "FILES_PER_RUN = 10\n",
    "LLM_MODEL = \"qwen2.5:14b\"\n",
    "LLM_TEMPERATURE = 0.5\n",
    "LLM_TOP_P = 0.9\n",
    "LLM_MAX_TOKENS = 8000  # Max tokens in LLM response\n",
    "TOKEN_BUFFER = 200  # Reserve tokens for prompt and mappings\n",
    "MAX_INPUT_TOKENS = LLM_MAX_TOKENS - TOKEN_BUFFER  # Max tokens for script content\n",
    "DEFAULT_OLLAMA_PORT = \"11434\"\n",
    "TIMEOUT_SECONDS = 60*3\n",
    "\n",
    "# LLM Prompt Template\n",
    "REFACTOR_PROMPT = \"\"\"\n",
    "You are an expert Python developer tasked with refactoring a Python script chunk. Perform these tasks:\n",
    "\n",
    "1. **Add Docstrings**: Replace all existing comments with detailed docstrings for the module, functions, and classes. Follow PEP 257: include purpose, parameters (type and description), return values, and side effects. Use triple quotes (\\\"\\\"\\\" or ''') for docstrings; retain # only for inline notes not suited to docstrings.\n",
    "\n",
    "2. **Improve Formatting**: Reformat the code adhering to Black's style (e.g., 88-character line length, consistent indentation, sorted imports alphabetically). Ensure readability and consistency.\n",
    "\n",
    "3. **Optimize Code**: Enhance performance without sacrificing readabilityâ€”simplify logic, remove inefficiencies, and streamline operations where possible.\n",
    "\n",
    "4. **Enhance Error Handling**: Implement robust error handling with specific exceptions (e.g., ValueError, IOError) and actionable error messages instead of generic ones.\n",
    "\n",
    "5. **Cleanup Dependencies**: Remove unused imports, dead code, and any i18n references (e.g., gettext or similar).\n",
    "\n",
    "6. **Rename Functions and Variables**: For functions and variables defined in this chunk (not imported), convert names to snake_case and make them more descriptive. Provide mappings for renamed items.\n",
    "\n",
    "7. **Verify Documentation**: Ensure every function and class has complete, accurate, up-to-date docstrings.\n",
    "\n",
    "Here is the Python script chunk to refactor:\n",
    "\n",
    "{script_content}\n",
    "\n",
    "Return the refactored chunk followed by a list of renamed functions/variables in this format:\n",
    "<refactored code> ``` --- ``` <original_name> -> <new_name> ```\n",
    "\n",
    "If no renames occur, include an empty mapping section.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def estimate_tokens(text: str) -> int:\n",
    "    \"\"\"Roughly estimate the number of tokens in text (approx. 1 token per 4 chars).\"\"\"\n",
    "    return len(text) // 4 + 1  # Add 1 to account for small rounding errors\n",
    "\n",
    "\n",
    "def fetch_llm_response(\n",
    "    prompt: str,\n",
    "    model: str = LLM_MODEL,\n",
    "    temperature: float = LLM_TEMPERATURE,\n",
    "    top_p: float = LLM_TOP_P,\n",
    "    max_tokens: int = LLM_MAX_TOKENS\n",
    ") -> str:\n",
    "    \"\"\"Send a prompt to the LLM API and return the response.\"\"\"\n",
    "    if not prompt or not isinstance(prompt, str):\n",
    "        raise ValueError(\"Prompt must be a non-empty string\")\n",
    "\n",
    "    ollama_port = os.getenv(\"OLLAMA_PORT\", DEFAULT_OLLAMA_PORT)\n",
    "    url = f\"http://localhost:{ollama_port}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers, timeout=TIMEOUT_SECONDS)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"response\"]\n",
    "    except requests.RequestException as e:\n",
    "        raise requests.RequestException(f\"Failed to get response from LLM API: {e}\")\n",
    "\n",
    "\n",
    "def load_name_mapping(mapping_file: str) -> Dict[str, str]:\n",
    "    \"\"\"Load existing function/variable name mappings from a file.\"\"\"\n",
    "    mapping = {}\n",
    "    if os.path.exists(mapping_file):\n",
    "        with open(mapping_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith(\"#\") and \"->\" in line:\n",
    "                    original, new = [part.strip() for part in line.split(\"->\")]\n",
    "                    mapping[original] = new\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def save_name_mapping(mapping_file: str, mapping: Dict[str, str]):\n",
    "    \"\"\"Save updated function/variable name mappings to a file.\"\"\"\n",
    "    with open(mapping_file, \"w\") as f:\n",
    "        f.write(\"# function_variable_mapping.txt\\n\")\n",
    "        f.write(\"# Format: original_name -> new_name\\n\\n\")\n",
    "        for original, new in sorted(mapping.items()):\n",
    "            f.write(f\"{original} -> {new}\\n\")\n",
    "\n",
    "\n",
    "def load_processed_files(tracker_file: str) -> Set[str]:\n",
    "    \"\"\"Load the set of already processed file paths from a tracker file.\"\"\"\n",
    "    processed = set()\n",
    "    if os.path.exists(tracker_file):\n",
    "        with open(tracker_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith(\"#\"):\n",
    "                    processed.add(line)\n",
    "    return processed\n",
    "\n",
    "\n",
    "def save_processed_files(tracker_file: str, processed: Set[str]):\n",
    "    \"\"\"Save the set of processed file paths to a tracker file.\"\"\"\n",
    "    with open(tracker_file, \"w\") as f:\n",
    "        f.write(\"# processed_files.txt\\n\")\n",
    "        f.write(\"# Tracks files already refactored\\n\\n\")\n",
    "        for file_path in sorted(processed):\n",
    "            f.write(f\"{file_path}\\n\")\n",
    "\n",
    "\n",
    "def apply_existing_mapping(script_content: str, mapping: Dict[str, str]) -> str:\n",
    "    \"\"\"Apply existing name mappings to the script content, handling longer names first.\"\"\"\n",
    "    sorted_mapping = sorted(mapping.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "    for original, new in sorted_mapping:\n",
    "        script_content = re.sub(r'\\b' + re.escape(original) + r'\\b', new, script_content)\n",
    "    return script_content\n",
    "\n",
    "\n",
    "def extract_mapping_from_response(response: str) -> Tuple[str, Dict[str, str]]:\n",
    "    \"\"\"Extract refactored code and new name mappings from LLM response.\"\"\"\n",
    "    try:\n",
    "        code_section, mapping_section = response.split(\"---\\n\", 1)\n",
    "        refactored_code = code_section.strip(\"`\\n\")\n",
    "        new_mapping = {}\n",
    "        mapping_lines = mapping_section.strip(\"`\\n\").split(\"\\n\")\n",
    "        for line in mapping_lines:\n",
    "            line = line.strip()\n",
    "            if line and \"->\" in line:\n",
    "                original, new = [part.strip() for part in line.split(\"->\")]\n",
    "                new_mapping[original] = new\n",
    "        return refactored_code, new_mapping\n",
    "    except ValueError:\n",
    "        raise ValueError(\"LLM response format invalid: missing code or mapping section\")\n",
    "\n",
    "\n",
    "def split_script_into_chunks(script_content: str, max_tokens: int = MAX_INPUT_TOKENS) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split a script into chunks that fit within max_tokens, preserving function boundaries where possible.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "\n",
    "    # Parse the script into an AST to identify function definitions\n",
    "    try:\n",
    "        tree = ast.parse(script_content)\n",
    "        lines = script_content.splitlines()\n",
    "        function_starts = {node.lineno - 1: node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)}\n",
    "    except SyntaxError:\n",
    "        # Fallback to line-by-line splitting if AST parsing fails\n",
    "        lines = script_content.splitlines()\n",
    "        function_starts = {}\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        line_tokens = estimate_tokens(line)\n",
    "        # Check if adding this line exceeds the token limit\n",
    "        if current_token_count + line_tokens > max_tokens and current_chunk:\n",
    "            chunks.append(\"\\n\".join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_token_count = 0\n",
    "\n",
    "        # Start a new chunk at function boundaries if possible\n",
    "        if i in function_starts and current_chunk:\n",
    "            chunks.append(\"\\n\".join(current_chunk))\n",
    "            current_chunk = [line]\n",
    "            current_token_count = line_tokens\n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "            current_token_count += line_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\"\\n\".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_single_file(file_path: str, global_mapping: Dict[str, str]) -> bool:\n",
    "    \"\"\"Process a single file, handling large scripts by splitting into chunks.\"\"\"\n",
    "    try:\n",
    "        # Try UTF-8 first\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            original_content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback: try to detect or use 'latin1' as a last resort\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                raw = f.read()\n",
    "                detected = chardet.detect(raw)\n",
    "                encoding = detected[\"encoding\"] or \"latin1\"\n",
    "            with open(file_path, \"r\", encoding=encoding) as f:\n",
    "                original_content = f.read()\n",
    "            print(f\"Warning: {file_path} decoded with fallback encoding '{encoding}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding {file_path}: {e}\")\n",
    "            return False\n",
    "    except IOError as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Apply existing mappings first\n",
    "    content_with_mappings = apply_existing_mapping(original_content, global_mapping)\n",
    "    total_tokens = estimate_tokens(content_with_mappings) + estimate_tokens(REFACTOR_PROMPT)\n",
    "\n",
    "    if total_tokens <= MAX_INPUT_TOKENS:\n",
    "        # Small enough to process in one go\n",
    "        prompt = REFACTOR_PROMPT.format(script_content=content_with_mappings)\n",
    "        try:\n",
    "            response = fetch_llm_response(prompt)\n",
    "            refactored_code, new_mapping = extract_mapping_from_response(response)\n",
    "        except (requests.RequestException, ValueError) as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        # Split into chunks and process sequentially\n",
    "        print(f\"Script {file_path} exceeds {MAX_INPUT_TOKENS} tokens ({total_tokens}). Splitting into chunks...\")\n",
    "        chunks = split_script_into_chunks(content_with_mappings)\n",
    "        refactored_chunks = []\n",
    "        new_mapping = {}\n",
    "\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            print(f\"Processing chunk {i}/{len(chunks)} of {file_path}\")\n",
    "            prompt = REFACTOR_PROMPT.format(script_content=chunk)\n",
    "            try:\n",
    "                response = fetch_llm_response(prompt)\n",
    "                refactored_chunk, chunk_mapping = extract_mapping_from_response(response)\n",
    "                refactored_chunks.append(refactored_chunk)\n",
    "                new_mapping.update(chunk_mapping)\n",
    "                # Apply new mappings to remaining chunks\n",
    "                for j in range(i, len(chunks)):\n",
    "                    chunks[j] = apply_existing_mapping(chunks[j], chunk_mapping)\n",
    "            except (requests.RequestException, ValueError) as e:\n",
    "                print(f\"Error processing chunk {i} of {file_path}: {e}\")\n",
    "                return False\n",
    "\n",
    "        refactored_code = \"\\n\\n\".join(refactored_chunks)\n",
    "\n",
    "    # Update global mapping and save file\n",
    "    global_mapping.update(new_mapping)\n",
    "    try:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:  # Write back as UTF-8\n",
    "            f.write(refactored_code)\n",
    "        print(f\"Refactored and overwrote {file_path}\")\n",
    "        return True\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def apply_mapping_to_all_files(directory: str, mapping: Dict[str, str], processed: Set[str]):\n",
    "    \"\"\"Apply the updated mapping to all Python files in the directory, skipping specified folders.\"\"\"\n",
    "    python_files = get_python_files(directory)\n",
    "    for file_path in python_files:\n",
    "        try:\n",
    "            with open(file_path, \"r\") as f:\n",
    "                content = f.read()\n",
    "            updated_content = apply_existing_mapping(content, mapping)\n",
    "            if updated_content != content:\n",
    "                with open(file_path, \"w\") as f:\n",
    "                    f.write(updated_content)\n",
    "                print(f\"Updated mappings in {file_path}\")\n",
    "        except IOError as e:\n",
    "            print(f\"Error updating {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def get_python_files(directory: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Recursively find all Python files in the directory, excluding SKIP_FOLDERS.\n",
    "    \"\"\"\n",
    "    python_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        # Modify dirs in-place to skip specified folders\n",
    "        dirs[:] = [d for d in dirs if d not in SKIP_FOLDERS]\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                python_files.append(full_path)\n",
    "    return python_files\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to process up to FILES_PER_RUN scripts and propagate mappings with a progress bar.\"\"\"\n",
    "    if not os.path.exists(DEFAULT_DIRECTORY):\n",
    "        print(f\"Error: Directory {DEFAULT_DIRECTORY} does not exist.\")\n",
    "        return\n",
    "\n",
    "    global_mapping = load_name_mapping(NAME_MAPPING_FILE)\n",
    "    processed_files = load_processed_files(PROCESSED_FILES_TRACKER)\n",
    "\n",
    "    python_files = get_python_files(DEFAULT_DIRECTORY)\n",
    "    if not python_files:\n",
    "        print(f\"No Python files found in {DEFAULT_DIRECTORY} (excluding {SKIP_FOLDERS}).\")\n",
    "        return\n",
    "\n",
    "    remaining_files = [f for f in python_files if f not in processed_files]\n",
    "    if not remaining_files:\n",
    "        print(\"All files have already been processed.\")\n",
    "        return\n",
    "\n",
    "    files_to_process = remaining_files[:FILES_PER_RUN]\n",
    "    print(f\"Found {len(remaining_files)} unprocessed files. Processing {len(files_to_process)} this run.\")\n",
    "    print(f\"Skipping folders: {SKIP_FOLDERS}\")\n",
    "\n",
    "    # Use tqdm to display a progress bar\n",
    "    with tqdm(total=len(files_to_process), desc=\"Refactoring Files\", unit=\"file\") as pbar:\n",
    "        for i, file_path in enumerate(files_to_process, 1):\n",
    "            tqdm.write(f\"Processing file {i}/{len(files_to_process)}: {file_path}\")\n",
    "            if process_single_file(file_path, global_mapping):\n",
    "                processed_files.add(file_path)\n",
    "                apply_mapping_to_all_files(DEFAULT_DIRECTORY, global_mapping, processed_files)\n",
    "                save_name_mapping(NAME_MAPPING_FILE, global_mapping)\n",
    "                save_processed_files(PROCESSED_FILES_TRACKER, processed_files)\n",
    "            pbar.update(1)\n",
    "\n",
    "    remaining = len(remaining_files) - len(files_to_process)\n",
    "    print(f\"Processed {len(files_to_process)} files this run. {remaining} files remain unprocessed.\")\n",
    "    print(\"Refactoring batch complete! Check the results and run again for the next batch.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
