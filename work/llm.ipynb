{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 10 of 343 remaining files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring:  10%|█         | 1/10 [00:03<00:33,  3.73s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing C:\\Users\\harold.noble\\Desktop\\open-webui - Copy\\app\\src\\lib\\apis\\index.ts: Failed to fetch LLM response after 3 attempts: Error code: 413 - {'error': {'message': 'Request too large for model `deepseek-r1-distill-llama-70b` in organization `org_01j1qez3f7fj98shsdn8gf3qsm` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 7071, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring:  20%|██        | 2/10 [00:10<00:45,  5.65s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully refactored C:\\Users\\harold.noble\\Desktop\\open-webui - Copy\\app\\src\\lib\\apis\\audio\\index.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring:  30%|███       | 3/10 [00:20<00:52,  7.43s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully refactored C:\\Users\\harold.noble\\Desktop\\open-webui - Copy\\app\\src\\lib\\apis\\auths\\index.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring:  40%|████      | 4/10 [00:24<00:35,  6.00s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing C:\\Users\\harold.noble\\Desktop\\open-webui - Copy\\app\\src\\lib\\apis\\chats\\index.ts: Failed to fetch LLM response after 3 attempts: Error code: 413 - {'error': {'message': 'Request too large for model `deepseek-r1-distill-llama-70b` in organization `org_01j1qez3f7fj98shsdn8gf3qsm` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 6488, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring:  50%|█████     | 5/10 [00:50<01:07, 13.45s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully refactored C:\\Users\\harold.noble\\Desktop\\open-webui - Copy\\app\\src\\lib\\apis\\configs\\index.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring:  60%|██████    | 6/10 [01:11<01:03, 15.77s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully refactored C:\\Users\\harold.noble\\Desktop\\open-webui - Copy\\app\\src\\lib\\apis\\evaluations\\index.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring:  70%|███████   | 7/10 [01:15<00:36, 12.14s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully refactored C:\\Users\\harold.noble\\Desktop\\open-webui - Copy\\app\\src\\lib\\apis\\files\\index.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring:  80%|████████  | 8/10 [01:20<00:19,  9.67s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully refactored C:\\Users\\harold.noble\\Desktop\\open-webui - Copy\\app\\src\\lib\\apis\\folders\\index.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring:  90%|█████████ | 9/10 [01:42<00:13, 13.66s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully refactored C:\\Users\\harold.noble\\Desktop\\open-webui - Copy\\app\\src\\lib\\apis\\functions\\index.ts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring: 100%|██████████| 10/10 [01:48<00:00, 10.83s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully refactored C:\\Users\\harold.noble\\Desktop\\open-webui - Copy\\app\\src\\lib\\apis\\groups\\index.ts\n",
      "Completed batch of 10 files. 333 files remain.\n",
      "Run the script again to process the next batch.\n",
      "Check 'problematic_files.txt' for 2 files that need manual review.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# ./refactor_script.py\n",
    "\"\"\"\n",
    "Script to refactor Python, Svelte, and TypeScript files using LLM streaming.\n",
    "Processes 5 files per run, tracks progress in a file, and overwrites originals with refactored code.\n",
    "Supports skipping specified subfolders and focuses on docstrings, formatting, optimization,\n",
    "error handling, and cleanup. Logs problematic files to 'problematic_files.txt' for manual review.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import List, Set, Optional\n",
    "import ast\n",
    "import chardet\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration constants\n",
    "GROQ_API_KEY = \"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\"\n",
    "DEFAULT_DIRECTORY = r\"C:\\Users\\harold.noble\\Desktop\\open-webui - Copy\\app\\src\"\n",
    "SKIP_FOLDERS = frozenset({\".github\", \"code_helper\", \"ollama-0\"})\n",
    "PROCESSED_FILES_TRACKER = \"processed_files.txt\"\n",
    "PROBLEMATIC_FILES_LOG = \"problematic_files.txt\"  # File to log problematic files\n",
    "FILES_PER_RUN = 10\n",
    "LLM_MODEL = \"deepseek-r1-distill-llama-70b\"\n",
    "LLM_TEMPERATURE = 0.5\n",
    "LLM_TOP_P = 0.9\n",
    "TIMEOUT_SECONDS = 180\n",
    "MAX_RETRIES = 3\n",
    "MAX_TOKENS = 6000  # Token limit based on error message\n",
    "\n",
    "# LLM prompts\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert developer specializing in code refactoring, created to assist with optimizing code scripts. Your role is to refactor entire scripts while preserving their original functionality and function names.\n",
    "\n",
    "IMPORTANT: Your response must contain ONLY the refactored code with no explanations, comments about changes, or formatting markers. Do not include markdown code blocks, explanations, or anything else that is not part of the actual code.\n",
    "\"\"\"\n",
    "\n",
    "REFACTOR_PROMPT = \"\"\"\n",
    "Refactor this code (Python, Svelte, or TypeScript) with the following improvements:\n",
    "\n",
    "1. Remove all code related to these components and their derivatives:\n",
    "   - webhookUrl, LDAP, oauth, enable_community_sharing, ENABLE_CHANNELS, playground, nonLocalVoices, haptic, mobile\n",
    "   - All i18n related code and references\n",
    "\n",
    "2. Optimize performance while maintaining readability:\n",
    "   - Simplify logic flows\n",
    "   - Eliminate redundancies and inefficiencies\n",
    "   - Streamline operations\n",
    "\n",
    "3. Clean up the codebase:\n",
    "   - Remove unused imports and dead code\n",
    "\n",
    "4. Enhance error handling:\n",
    "   - Use specific exception types (ValueError, IOError, etc.)\n",
    "   - Include actionable error messages\n",
    "\n",
    "5. Improve documentation:\n",
    "   - Replace comments with appropriate docstrings for modules, functions, and classes\n",
    "   - Retain inline comments for notes not suitable for docstrings\n",
    "\n",
    "IMPORTANT: Do not rename any functions during refactoring. Return ONLY the refactored code without explanations, markdown formatting, or change summaries.\n",
    "\n",
    "Code:\n",
    "{code_content}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fetch_llm_response(\n",
    "    prompt: str,\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    model: str = LLM_MODEL,\n",
    "    temperature: float = LLM_TEMPERATURE,\n",
    "    top_p: float = LLM_TOP_P,\n",
    "    retries: int = MAX_RETRIES,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Fetch refactored code from the Groq API.\n",
    "\n",
    "    Args:\n",
    "        prompt: The input prompt with code to refactor.\n",
    "        system_prompt: Instructions for the LLM.\n",
    "        model: The LLM model to use.\n",
    "        temperature: Sampling temperature for LLM output.\n",
    "        top_p: Top-p sampling parameter for LLM output.\n",
    "        retries: Number of retry attempts on failure.\n",
    "\n",
    "    Returns:\n",
    "        The refactored code as a string.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If prompt is invalid or API key is unset.\n",
    "        RuntimeError: If API call fails after all retries.\n",
    "    \"\"\"\n",
    "    if not prompt or not isinstance(prompt, str):\n",
    "        raise ValueError(\"Prompt must be a non-empty string\")\n",
    "    if len(prompt) > MAX_TOKENS * 4:  # Rough estimate: 4 chars per token\n",
    "        raise ValueError(\"Prompt exceeds token limit\")\n",
    "    if not GROQ_API_KEY or GROQ_API_KEY == \"type_your_api_key_here\":\n",
    "        raise ValueError(\"Valid GROQ_API_KEY environment variable or constant required\")\n",
    "\n",
    "    client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                stream=False,\n",
    "                timeout=TIMEOUT_SECONDS,\n",
    "            )\n",
    "            return extract_code(chat_completion.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                raise RuntimeError(f\"Failed to fetch LLM response after {retries} attempts: {str(e)}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "\n",
    "\n",
    "def extract_code(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract pure code from LLM response, stripping non-code content.\n",
    "\n",
    "    Args:\n",
    "        response: Raw LLM response string.\n",
    "\n",
    "    Returns:\n",
    "        The extracted code string.\n",
    "    \"\"\"\n",
    "    code_block_pattern = r\"```(?:python|typescript|svelte|js|jsx|ts|tsx)?(.*?)```\"\n",
    "    code_blocks = re.findall(code_block_pattern, response, re.DOTALL)\n",
    "    if code_blocks:\n",
    "        return \"\\n\\n\".join(block.strip() for block in code_blocks)\n",
    "\n",
    "    lines = response.split('\\n')\n",
    "    code_lines = [line for line in lines if not any(\n",
    "        marker.lower() in line.lower() for marker in (\n",
    "            \"here's the refactored\", \"explanation:\", \"summary of changes:\"\n",
    "        )\n",
    "    )]\n",
    "    return \"\\n\".join(code_lines) or response\n",
    "\n",
    "\n",
    "def load_processed_files(tracker_file: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Load previously processed file paths from the tracker file, creating it if it doesn't exist.\n",
    "\n",
    "    Args:\n",
    "        tracker_file: Path to the processed files tracker.\n",
    "\n",
    "    Returns:\n",
    "        Set of processed file paths.\n",
    "    \"\"\"\n",
    "    processed = set()\n",
    "    try:\n",
    "        if not os.path.exists(tracker_file):\n",
    "            with open(tracker_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"# Processed Files\\n# Tracks refactored files\\n\\n\")\n",
    "            print(f\"Created new tracker file: {tracker_file}\")\n",
    "        else:\n",
    "            with open(tracker_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                processed.update(\n",
    "                    line.strip() for line in f\n",
    "                    if line.strip() and not line.startswith(\"#\")\n",
    "                )\n",
    "    except IOError as e:\n",
    "        print(f\"Error: Could not access {tracker_file}: {e}\")\n",
    "    return processed\n",
    "\n",
    "\n",
    "def save_processed_files(tracker_file: str, processed: Set[str]) -> None:\n",
    "    \"\"\"\n",
    "    Save processed file paths to the tracker file.\n",
    "\n",
    "    Args:\n",
    "        tracker_file: Path to the tracker file.\n",
    "        processed: Set of processed file paths.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(tracker_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"# Processed Files\\n# Tracks refactored files\\n\\n\")\n",
    "            f.writelines(f\"{path}\\n\" for path in sorted(processed))\n",
    "    except IOError as e:\n",
    "        print(f\"Error: Failed to save {tracker_file}: {e}\")\n",
    "\n",
    "\n",
    "def load_problematic_files(log_file: str) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Load previously logged problematic file paths, creating the file if it doesn't exist.\n",
    "\n",
    "    Args:\n",
    "        log_file: Path to the problematic files log.\n",
    "\n",
    "    Returns:\n",
    "        Set of problematic file paths.\n",
    "    \"\"\"\n",
    "    problematic = set()\n",
    "    try:\n",
    "        if not os.path.exists(log_file):\n",
    "            with open(log_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"# Problematic Files\\n# Tracks files that failed refactoring\\n\\n\")\n",
    "            print(f\"Created new problematic files log: {log_file}\")\n",
    "        else:\n",
    "            with open(log_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                problematic.update(\n",
    "                    line.strip() for line in f\n",
    "                    if line.strip() and not line.startswith(\"#\")\n",
    "                )\n",
    "    except IOError as e:\n",
    "        print(f\"Error: Could not access {log_file}: {e}\")\n",
    "    return problematic\n",
    "\n",
    "\n",
    "def save_problematic_files(log_file: str, problematic: Set[str]) -> None:\n",
    "    \"\"\"\n",
    "    Save problematic file paths to the log file.\n",
    "\n",
    "    Args:\n",
    "        log_file: Path to the problematic files log.\n",
    "        problematic: Set of problematic file paths.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(log_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"# Problematic Files\\n# Tracks files that failed refactoring\\n\\n\")\n",
    "            f.writelines(f\"{path}\\n\" for path in sorted(problematic))\n",
    "    except IOError as e:\n",
    "        print(f\"Error: Failed to save {log_file}: {e}\")\n",
    "\n",
    "\n",
    "def is_valid_code(file_path: str, code: str) -> bool:\n",
    "    \"\"\"\n",
    "    Validate code syntax based on file type.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the file.\n",
    "        code: Code content to validate.\n",
    "\n",
    "    Returns:\n",
    "        True if code is syntactically valid, False otherwise.\n",
    "    \"\"\"\n",
    "    if file_path.endswith(\".py\"):\n",
    "        try:\n",
    "            ast.parse(code)\n",
    "            return True\n",
    "        except SyntaxError:\n",
    "            return False\n",
    "    return bool(code.strip())\n",
    "\n",
    "\n",
    "def process_single_file(file_path: str, problematic_files: Set[str]) -> bool:\n",
    "    \"\"\"\n",
    "    Refactor a single file and overwrite it if successful, log to problematic files if it fails.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the file to process.\n",
    "        problematic_files: Set of files that failed processing.\n",
    "\n",
    "    Returns:\n",
    "        True if refactoring succeeded and file was updated, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = \"utf-8\"\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=encoding) as f:\n",
    "                original_content = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                raw = f.read()\n",
    "                detected = chardet.detect(raw)\n",
    "                encoding = detected[\"encoding\"] or \"latin1\"\n",
    "            with open(file_path, \"r\", encoding=encoding) as f:\n",
    "                original_content = f.read()\n",
    "            tqdm.write(f\"Note: {file_path} read with encoding '{encoding}'\")\n",
    "\n",
    "        prompt = REFACTOR_PROMPT.format(code_content=original_content)\n",
    "        try:\n",
    "            refactored_content = fetch_llm_response(prompt)\n",
    "        except (ValueError, RuntimeError) as e:\n",
    "            tqdm.write(f\"Error processing {file_path}: {str(e)}\")\n",
    "            problematic_files.add(file_path)\n",
    "            return False\n",
    "\n",
    "        if not is_valid_code(file_path, refactored_content):\n",
    "            tqdm.write(f\"Error: Invalid refactored code for {file_path}\")\n",
    "            problematic_files.add(file_path)\n",
    "            return False\n",
    "\n",
    "        if refactored_content.strip() == original_content.strip():\n",
    "            tqdm.write(f\"Note: No changes needed for {file_path}\")\n",
    "            return False\n",
    "\n",
    "        refactored_content = refactored_content.rstrip() + \"\\n\"\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(refactored_content)\n",
    "        tqdm.write(f\"Successfully refactored {file_path}\")\n",
    "        return True\n",
    "\n",
    "    except IOError as e:\n",
    "        tqdm.write(f\"Error processing {file_path}: {str(e)}\")\n",
    "        problematic_files.add(file_path)\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_files_to_process(directory: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find all target files in the directory, excluding skipped folders.\n",
    "\n",
    "    Args:\n",
    "        directory: Root directory to search.\n",
    "\n",
    "    Returns:\n",
    "        List of file paths to process.\n",
    "    \"\"\"\n",
    "    extensions = (\".py\", \".svelte\", \".ts\", \".tsx\")\n",
    "    return [\n",
    "        os.path.join(root, file)\n",
    "        for root, dirs, files in os.walk(directory)\n",
    "        if not any(skip in root for skip in SKIP_FOLDERS)\n",
    "        for file in files\n",
    "        if file.endswith(extensions)\n",
    "    ]\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to orchestrate file refactoring, processing 5 files per run with progress tracking.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(DEFAULT_DIRECTORY):\n",
    "        print(f\"Error: Directory '{DEFAULT_DIRECTORY}' not found\")\n",
    "        return\n",
    "\n",
    "    processed_files = load_processed_files(PROCESSED_FILES_TRACKER)\n",
    "    problematic_files = load_problematic_files(PROBLEMATIC_FILES_LOG)\n",
    "    files_to_process = get_files_to_process(DEFAULT_DIRECTORY)\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(f\"No target files found in {DEFAULT_DIRECTORY}\")\n",
    "        return\n",
    "\n",
    "    remaining_files = [f for f in files_to_process if f not in processed_files]\n",
    "    if not remaining_files:\n",
    "        print(\"All files already processed\")\n",
    "        return\n",
    "\n",
    "    files_this_run = remaining_files[:FILES_PER_RUN]\n",
    "    print(f\"Processing {len(files_this_run)} of {len(remaining_files)} remaining files\")\n",
    "\n",
    "    with tqdm(total=len(files_this_run), desc=\"Refactoring\", unit=\"file\") as pbar:\n",
    "        for file_path in files_this_run:\n",
    "            if process_single_file(file_path, problematic_files):\n",
    "                processed_files.add(file_path)\n",
    "                save_processed_files(PROCESSED_FILES_TRACKER, processed_files)\n",
    "            if file_path in problematic_files:\n",
    "                save_problematic_files(PROBLEMATIC_FILES_LOG, problematic_files)\n",
    "            pbar.update(1)\n",
    "\n",
    "    remaining = len(remaining_files) - len(files_this_run)\n",
    "    print(f\"Completed batch of {len(files_this_run)} files. {remaining} files remain.\")\n",
    "    if remaining > 0:\n",
    "        print(\"Run the script again to process the next batch.\")\n",
    "    if problematic_files:\n",
    "        print(f\"Check '{PROBLEMATIC_FILES_LOG}' for {len(problematic_files)} files that need manual review.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
