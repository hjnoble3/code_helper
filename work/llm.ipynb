{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 unprocessed files. Processing 1 this run.\n",
      "Skipping folders: {'ollama-0', 'code_helper', '.github'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring Files:   0%|          | 0/1 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/1: C:\\Users\\harold.noble\\Desktop\\open-webui\\app\\backend\\webui\\storage\\provider.py\n",
      "Found 7 top-level chunks in C:\\Users\\harold.noble\\Desktop\\open-webui\\app\\backend\\webui\\storage\\provider.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactoring Files:   0%|          | 0/1 [00:00<?, ?file/s]       \n",
      "Refactoring Files:   0%|          | 0/1 [00:00<?, ?file/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping non-function/class chunk 1/7 (lines 1-31)\n",
      "Processing chunk 2/7 (lines 32-49)\n",
      "Streaming LLM response: ```python\n",
      "from abc import ABC, abstractmethod\n",
      "from typing import BinaryIO, Tuple\n",
      "\n",
      "\n",
      "class storage_provider(ABC):\n",
      "    \"\"\"Abstract base class for storage providers.\"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    def get_file(self, file_path: str) -> str:\n",
      "        \"\"\"Retrieve a file from the storage.\n",
      "\n",
      "        :param file_path: Path to the file.\n",
      "        :return: Content of the file.\n",
      "        :raises FileNotFoundError: If the file does not exist.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def upload_file(\n",
      "        self, file: BinaryIO, filename: str\n",
      "    ) -> Tuple[bytes, str]:\n",
      "        \"\"\"Upload a file to the storage.\n",
      "\n",
      "        :param file: File object containing the data to be uploaded.\n",
      "        :param filename: Name of the file in the storage.\n",
      "        :return: A tuple containing the file's bytes and its path in the storage.\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def delete_all_files(self) -> None:\n",
      "        \"\"\"Delete all files from the storage.\"\"\"\n",
      "        pass\n",
      "\n",
      "    @abstractmethod\n",
      "    def delete_file(self, file_path: str) -> None:\n",
      "        \"\"\"Delete a specific file from the storage.\n",
      "\n",
      "        :param file_path: Path to the file to be deleted.\n",
      "        :raises FileNotFoundError: If the file does not exist.\n",
      "        \"\"\"\n",
      "        pass\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactoring Files:   0%|          | 0/1 [00:08<?, ?file/s]                \n",
      "Refactoring Files:   0%|          | 0/1 [00:08<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:08<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:08<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:08<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:08<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:08<?, ?file/s]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactored chunk preview (first few lines):\n",
      "  from abc import ABC, abstractmethod\n",
      "  from typing import BinaryIO, Tuple\n",
      "  \n",
      "  \n",
      "  class storage_provider(ABC):\n",
      "  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring Files:   0%|          | 0/1 [00:08<?, ?file/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 3/7 (lines 50-92)\n",
      "Streaming LLM response: ```python\n",
      "import os\n",
      "import shutil\n",
      "from typing import BinaryIO, Tuple\n",
      "\n",
      "class LocalStorageProvider:\n",
      "    @staticmethod\n",
      "    def upload_file(file: BinaryIO, filename: str) -> Tuple[bytes, str]:\n",
      "        \"\"\"Uploads a file to the local storage.\n",
      "\n",
      "        Args:\n",
      "            file (BinaryIO): The binary stream of the file.\n",
      "            filename (str): The name of the file.\n",
      "\n",
      "        Returns:\n",
      "            Tuple[bytes, str]: A tuple containing the file contents and the file path.\n",
      "\n",
      "        Raises:\n",
      "            ValueError: If the file content is empty.\n",
      "        \"\"\"\n",
      "        contents = file.read()\n",
      "        if not contents:\n",
      "            raise ValueError(\"File content is empty.\")\n",
      "        file_path = os.path.join(UPLOAD_DIR, filename)\n",
      "        with open(file_path, \"wb\") as f:\n",
      "            f.write(contents)\n",
      "        return contents, file_path\n",
      "\n",
      "    @staticmethod\n",
      "    def get_file(file_path: str) -> str:\n",
      "        \"\"\"Returns the path of the file from local storage.\n",
      "\n",
      "        Args:\n",
      "            file_path (str): The path to the file.\n",
      "\n",
      "        Returns:\n",
      "            str: The file path.\n",
      "        \"\"\"\n",
      "        return file_path\n",
      "\n",
      "    @staticmethod\n",
      "    def delete_file(file_path: str) -> None:\n",
      "        \"\"\"Deletes a file from local storage.\n",
      "\n",
      "        Args:\n",
      "            file_path (str): The path to the file.\n",
      "\n",
      "        Raises:\n",
      "            FileNotFoundError: If the file is not found in local storage.\n",
      "        \"\"\"\n",
      "        filename = os.path.basename(file_path)\n",
      "        full_file_path = os.path.join(UPLOAD_DIR, filename)\n",
      "        if os.path.isfile(full_file_path):\n",
      "            os.remove(full_file_path)\n",
      "        else:\n",
      "            raise FileNotFoundError(f\"File {full_file_path} not found in local storage.\")\n",
      "\n",
      "    @staticmethod\n",
      "    def delete_all_files() -> None:\n",
      "        \"\"\"Deletes all files and directories from the local storage.\n",
      "\n",
      "        Raises:\n",
      "            IOError: If an error occurs during deletion.\n",
      "        \"\"\"\n",
      "        if os.path.exists(UPLOAD_DIR):\n",
      "            for item_name in os.listdir(UPLOAD_DIR):\n",
      "                item_path = os.path.join(UPLOAD_DIR, item_name)\n",
      "                try:\n",
      "                    if os.path.isfile(item_path) or os.path.islink(item_path):\n",
      "                        os.unlink(item_path)\n",
      "                    elif os.path.isdir(item_path):\n",
      "                        shutil.rmtree(item_path)\n",
      "                except Exception as e:\n",
      "                    raise IOError(f\"Failed to delete {item_path}. Reason: {e}\")\n",
      "        else:\n",
      "            raise FileNotFoundError(f\"Directory {UPLOAD_DIR} not found in local storage.\")\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactoring Files:   0%|          | 0/1 [00:24<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:24<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:24<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:24<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:24<?, ?file/s]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactored chunk preview (first few lines):\n",
      "  import os\n",
      "  import shutil\n",
      "  from typing import BinaryIO, Tuple\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring Files:   0%|          | 0/1 [00:24<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:24<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:24<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:24<?, ?file/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class LocalStorageProvider:\n",
      "  ...\n",
      "Error: Combined code after chunk 3 is invalid Python. Reverting this chunk.\n",
      "Processing chunk 4/7 (lines 93-165)\n",
      "Streaming LLM response: ```python\n",
      "import os\n",
      "from typing import BinaryIO, Tuple\n",
      "import boto3\n",
      "from botocore.exceptions import ClientError\n",
      "\n",
      "class S3StorageProvider(StorageProvider):\n",
      "    def __init__(self):\n",
      "        self.s3_client = boto3.client(\n",
      "            \"s3\",\n",
      "            region_name=S3_REGION_NAME,\n",
      "            endpoint_url=S3_ENDPOINT_URL,\n",
      "            aws_access_key_id=S3_ACCESS_KEY_ID,\n",
      "            aws_secret_access_key=S3_SECRET_ACCESS_KEY,\n",
      "        )\n",
      "        self.bucket_name = S3_BUCKET_NAME\n",
      "        self.key_prefix = S3_KEY_PREFIX if S3_KEY_PREFIX else \"\"\n",
      "\n",
      "    def upload_file(self, file: BinaryIO, filename: str) -> Tuple[bytes, str]:\n",
      "        \"\"\"Uploads a file to S3 storage.\"\"\"\n",
      "        _, file_path = LocalStorageProvider.upload_file(file, filename)\n",
      "        s3_key = os.path.join(self.key_prefix, filename)\n",
      "        try:\n",
      "            self.s3_client.upload_file(file_path, self.bucket_name, s3_key)\n",
      "            with open(file_path, \"rb\") as f:\n",
      "                return f.read(), f\"s3://{self.bucket_name}/{s3_key}\"\n",
      "        except ClientError as e:\n",
      "            raise RuntimeError(f\"Failed to upload file to S3: {e}\")\n",
      "\n",
      "    def get_file(self, file_path: str) -> str:\n",
      "        \"\"\"Downloads a file from S3 storage.\"\"\"\n",
      "        try:\n",
      "            s3_key = self._extract_s3_key(file_path)\n",
      "            local_file_path = self._get_local_file_path(s3_key)\n",
      "            self.s3_client.download_file(self.bucket_name, s3_key, local_file_path)\n",
      "            return local_file_path\n",
      "        except ClientError as e:\n",
      "            raise RuntimeError(f\"Failed to download file from S3: {e}\")\n",
      "\n",
      "    def delete_file(self, file_path: str) -> None:\n",
      "        \"\"\"Deletes a file from S3 storage.\"\"\"\n",
      "        try:\n",
      "            s3_key = self._extract_s3_key(file_path)\n",
      "            self.s3_client.delete_object(Bucket=self.bucket_name, Key=s3_key)\n",
      "        except ClientError as e:\n",
      "            raise RuntimeError(f\"Failed to delete file from S3: {e}\")\n",
      "\n",
      "        # Always delete from local storage\n",
      "        LocalStorageProvider.delete_file(file_path)\n",
      "\n",
      "    def delete_all_files(self) -> None:\n",
      "        \"\"\"Deletes all files from S3 storage.\"\"\"\n",
      "        try:\n",
      "            response = self.s3_client.list_objects_v2(Bucket=self.bucket_name)\n",
      "            if \"Contents\" in response:\n",
      "                for content in response[\"Contents\"]:\n",
      "                    if not content[\"Key\"].startswith(self.key_prefix):\n",
      "                        continue\n",
      "                    self.s3_client.delete_object(\n",
      "                        Bucket=self.bucket_name, Key=content[\"Key\"]\n",
      "                    )\n",
      "        except ClientError as e:\n",
      "            raise RuntimeError(f\"Failed to delete all files from S3: {e}\")\n",
      "\n",
      "        # Always delete from local storage\n",
      "        LocalStorageProvider.delete_all_files()\n",
      "\n",
      "    def _extract_s3_key(self, full_file_path: str) -> str:\n",
      "        return \"/\".join(full_file_path.split(\"//\")[1].split(\"/\")[1:])\n",
      "\n",
      "    def _get_local_file_path(self, s3_key: str) -> str:\n",
      "        return f\"{UPLOAD_DIR}/{s3_key.split('/')[-1]}\"\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactoring Files:   0%|          | 0/1 [00:44<?, ?file/s]               \n",
      "                                                                         \n",
      "Refactoring provider.py:  29%|██▊       | 2/7 [00:44<00:22,  4.44s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactored chunk preview (first few lines):\n",
      "  import os\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring Files:   0%|          | 0/1 [00:44<?, ?file/s]\n",
      "Refactoring Files:   0%|          | 0/1 [00:44<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:44<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:44<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:44<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:44<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [00:44<?, ?file/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  from typing import BinaryIO, Tuple\n",
      "  import boto3\n",
      "  from botocore.exceptions import ClientError\n",
      "  \n",
      "  ...\n",
      "Error: Combined code after chunk 4 is invalid Python. Reverting this chunk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactoring Files:   0%|          | 0/1 [00:44<?, ?file/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 5/7 (lines 166-229)\n",
      "Streaming LLM response: ```python\n",
      "from typing import BinaryIO, Tuple\n",
      "import json\n",
      "from google.cloud import storage\n",
      "from google.api_core.exceptions import NotFound\n",
      "\n",
      "class gcs_storage_provider(StorageProvider):\n",
      "    def __init__(self):\n",
      "        self.bucket_name = GCS_BUCKET_NAME\n",
      "\n",
      "        if GOOGLE_APPLICATION_CREDENTIALS_JSON:\n",
      "            self.gcs_client = storage.Client.from_service_account_info(\n",
      "                info=json.loads(GOOGLE_APPLICATION_CREDENTIALS_JSON)\n",
      "            )\n",
      "        else:\n",
      "            # If no credentials json is provided, credentials will be picked up from the environment.\n",
      "            # If running on local environment, credentials would be user credentials.\n",
      "            # If running on a Compute Engine instance, credentials would be from Google Metadata server.\n",
      "            self.gcs_client = storage.Client()\n",
      "        self.bucket = self.gcs_client.bucket(GCS_BUCKET_NAME)\n",
      "\n",
      "    def upload_file(self, file: BinaryIO, filename: str) -> Tuple[bytes, str]:\n",
      "        \"\"\"Uploads a file to GCS storage.\"\"\"\n",
      "        contents, file_path = LocalStorageProvider.upload_file(file, filename)\n",
      "        try:\n",
      "            blob = self.bucket.blob(filename)\n",
      "            blob.upload_from_filename(file_path)\n",
      "            return contents, f\"gs://{self.bucket_name}/{filename}\"\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Error uploading file to GCS: {e}\")\n",
      "\n",
      "    def get_file(self, file_path: str) -> str:\n",
      "        \"\"\"Downloads a file from GCS storage.\"\"\"\n",
      "        try:\n",
      "            filename = file_path.removeprefix(\"gs://\").split(\"/\")[1]\n",
      "            local_file_path = f\"{UPLOAD_DIR}/{filename}\"\n",
      "            blob = self.bucket.get_blob(filename)\n",
      "            if not blob:\n",
      "                raise NotFound(f\"Blob {filename} not found.\")\n",
      "            blob.download_to_filename(local_file_path)\n",
      "\n",
      "            return local_file_path\n",
      "        except NotFound as e:\n",
      "            raise RuntimeError(f\"Error downloading file from GCS: {e}\")\n",
      "\n",
      "    def delete_file(self, file_path: str) -> None:\n",
      "        \"\"\"Deletes a file from GCS storage.\"\"\"\n",
      "        try:\n",
      "            filename = file_path.removeprefix(\"gs://\").split(\"/\")[1]\n",
      "            blob = self.bucket.get_blob(filename)\n",
      "            if not blob:\n",
      "                raise NotFound(f\"Blob {filename} not found.\")\n",
      "            blob.delete()\n",
      "        except NotFound as e:\n",
      "            raise RuntimeError(f\"Error deleting file from GCS: {e}\")\n",
      "\n",
      "        # Always delete from local storage\n",
      "        LocalStorageProvider.delete_file(file_path)\n",
      "\n",
      "    def delete_all_files(self) -> None:\n",
      "        \"\"\"Deletes all files from GCS storage.\"\"\"\n",
      "        try:\n",
      "            blobs = self.bucket.list_blobs()\n",
      "\n",
      "            for blob in blobs:\n",
      "                blob.delete()\n",
      "\n",
      "        except NotFound as e:\n",
      "            raise RuntimeError(f\"Error deleting all files from GCS: {e}\")\n",
      "\n",
      "        # Always delete from local storage\n",
      "        LocalStorageProvider.delete_all_files()\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactoring Files:   0%|          | 0/1 [01:03<?, ?file/s]               \n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactored chunk preview (first few lines):\n",
      "  from typing import BinaryIO, Tuple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring Files:   0%|          | 0/1 [01:03<?, ?file/s]\n",
      "Refactoring Files:   0%|          | 0/1 [01:03<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:03<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:03<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:03<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:03<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:03<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:03<?, ?file/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  import json\n",
      "  from google.cloud import storage\n",
      "  from google.api_core.exceptions import NotFound\n",
      "  \n",
      "  ...\n",
      "Error: Combined code after chunk 5 is invalid Python. Reverting this chunk.\n",
      "Processing chunk 6/7 (lines 230-297)\n",
      "Streaming LLM response: ```python\n",
      "from azure.storage.blob import BlobServiceClient, DefaultAzureCredential, ResourceNotFoundError\n",
      "from typing import BinaryIO, Tuple\n",
      "\n",
      "class azure_storage_provider(StorageProvider):\n",
      "    def __init__(self):\n",
      "        self.endpoint = AZURE_STORAGE_ENDPOINT\n",
      "        self.container_name = AZURE_STORAGE_CONTAINER_NAME\n",
      "        storage_key = AZURE_STORAGE_KEY\n",
      "\n",
      "        if storage_key:\n",
      "            self.blob_service_client = BlobServiceClient(account_url=self.endpoint, credential=storage_key)\n",
      "        else:\n",
      "            self.blob_service_client = BlobServiceClient(\n",
      "                account_url=self.endpoint, credential=DefaultAzureCredential()\n",
      "            )\n",
      "        self.container_client = self.blob_service_client.get_container_client(self.container_name)\n",
      "\n",
      "    def upload_file(self, file: BinaryIO, filename: str) -> Tuple[bytes, str]:\n",
      "        \"\"\"Uploads a file to Azure Blob Storage.\"\"\"\n",
      "        contents, _ = LocalStorageProvider.upload_file(file, filename)\n",
      "        try:\n",
      "            blob_client = self.container_client.get_blob_client(filename)\n",
      "            blob_client.upload_blob(contents, overwrite=True)\n",
      "            return contents, f\"{self.endpoint}/{self.container_name}/{filename}\"\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Error uploading file to Azure Blob Storage: {e}\")\n",
      "\n",
      "    def get_file(self, file_path: str) -> str:\n",
      "        \"\"\"Downloads a file from Azure Blob Storage.\"\"\"\n",
      "        try:\n",
      "            filename = file_path.split(\"/\")[-1]\n",
      "            local_file_path = f\"{UPLOAD_DIR}/{filename}\"\n",
      "            blob_client = self.container_client.get_blob_client(filename)\n",
      "            with open(local_file_path, \"wb\") as download_file:\n",
      "                download_file.write(blob_client.download_blob().readall())\n",
      "            return local_file_path\n",
      "        except ResourceNotFoundError as e:\n",
      "            raise RuntimeError(f\"Error downloading file from Azure Blob Storage: {e}\")\n",
      "\n",
      "    def delete_file(self, file_path: str) -> None:\n",
      "        \"\"\"Deletes a file from Azure Blob Storage.\"\"\"\n",
      "        try:\n",
      "            filename = file_path.split(\"/\")[-1]\n",
      "            blob_client = self.container_client.get_blob_client(filename)\n",
      "            blob_client.delete_blob()\n",
      "        except ResourceNotFoundError as e:\n",
      "            raise RuntimeError(f\"Error deleting file from Azure Blob Storage: {e}\")\n",
      "\n",
      "        LocalStorageProvider.delete_file(file_path)\n",
      "\n",
      "    def delete_all_files(self) -> None:\n",
      "        \"\"\"Deletes all files from Azure Blob Storage.\"\"\"\n",
      "        try:\n",
      "            blobs = self.container_client.list_blobs()\n",
      "            for blob in blobs:\n",
      "                self.container_client.delete_blob(blob.name)\n",
      "        except Exception as e:\n",
      "            raise RuntimeError(f\"Error deleting all files from Azure Blob Storage: {e}\")\n",
      "\n",
      "        LocalStorageProvider.delete_all_files()\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactoring Files:   0%|          | 0/1 [01:20<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:20<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:20<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:20<?, ?file/s]               \n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactored chunk preview (first few lines):\n",
      "  from azure.storage.blob import BlobServiceClient, DefaultAzureCredential, ResourceNotFoundError\n",
      "  from typing import BinaryIO, Tuple\n",
      "  \n",
      "  class azure_storage_provider(StorageProvider):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring Files:   0%|          | 0/1 [01:20<?, ?file/s]\n",
      "Refactoring Files:   0%|          | 0/1 [01:20<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:20<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:20<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:20<?, ?file/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      def __init__(self):\n",
      "  ...\n",
      "Error: Combined code after chunk 6 is invalid Python. Reverting this chunk.\n",
      "Processing chunk 7/7 (lines 298-312)\n",
      "Streaming LLM response: ```python\n",
      "from typing import Union\n",
      "\n",
      "class LocalStorageProvider:\n",
      "    pass\n",
      "\n",
      "class S3StorageProvider:\n",
      "    pass\n",
      "\n",
      "class GCSStorageProvider:\n",
      "    pass\n",
      "\n",
      "class AzureStorageProvider:\n",
      "    pass\n",
      "\n",
      "def get_storage_provider(storage_provider: str) -> Union[\n",
      "    LocalStorageProvider, \n",
      "    S3StorageProvider, \n",
      "    GCSStorageProvider, \n",
      "    AzureStorageProvider\n",
      "]:\n",
      "    \"\"\"\n",
      "    Returns an instance of the storage provider based on the specified type.\n",
      "\n",
      "    Args:\n",
      "        storage_provider (str): The type of storage provider ('local', 's3', 'gcs', 'azure').\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If the provided storage provider is not supported.\n",
      "    \"\"\"\n",
      "    providers = {\n",
      "        \"local\": LocalStorageProvider,\n",
      "        \"s3\": S3StorageProvider,\n",
      "        \"gcs\": GCSStorageProvider,\n",
      "        \"azure\": AzureStorageProvider\n",
      "    }\n",
      "    \n",
      "    if storage_provider not in providers:\n",
      "        raise ValueError(f\"Unsupported storage provider: {storage_provider}\")\n",
      "    \n",
      "    return providers[storage_provider]()\n",
      "\n",
      "# Example usage\n",
      "STORAGE_PROVIDER = \"local\"\n",
      "storage = get_storage_provider(STORAGE_PROVIDER)\n",
      "```"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactoring Files:   0%|          | 0/1 [01:28<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:28<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:28<?, ?file/s]               \n",
      "                                                                         \n",
      "Refactoring provider.py:  29%|██▊       | 2/7 [01:28<00:22,  4.44s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactored chunk preview (first few lines):\n",
      "  from typing import Union\n",
      "  \n",
      "  class LocalStorageProvider:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring Files:   0%|          | 0/1 [01:29<?, ?file/s]\n",
      "Refactoring Files:   0%|          | 0/1 [01:29<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:29<?, ?file/s]               \n",
      "Refactoring Files:   0%|          | 0/1 [01:29<?, ?file/s]               \n",
      "Refactoring provider.py:  29%|██▊       | 2/7 [01:29<03:42, 44.51s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pass\n",
      "  \n",
      "  ...\n",
      "Error: Combined code after chunk 7 is invalid Python. Reverting this chunk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Refactoring Files: 100%|██████████| 1/1 [01:29<00:00, 89.04s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refactored and overwrote C:\\Users\\harold.noble\\Desktop\\open-webui\\app\\backend\\webui\\storage\\provider.py\n",
      "Processed 1 files this run. 0 files remain unprocessed.\n",
      "Refactoring batch complete! Check the results and run again for the next batch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Script to refactor Python files by processing top-level 'def' or 'class' chunks one at a time with LLM streaming.\n",
    "Tracks processed files and saves refactored code back to the original files.\n",
    "Allows skipping specified subfolders. Focuses on docstrings, formatting, optimization, error handling, and cleanup.\n",
    "\"\"\"\n",
    "\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from typing import List, Set, Tuple\n",
    "\n",
    "import chardet\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants\n",
    "DEFAULT_DIRECTORY = r\"C:\\Users\\harold.noble\\Desktop\\open-webui\\app\\backend\\webui\\storage\"\n",
    "SKIP_FOLDERS = {\".github\", \"code_helper\", \"ollama-0\"}\n",
    "PROCESSED_FILES_TRACKER = \"processed_files.txt\"\n",
    "FILES_PER_RUN = 10\n",
    "LLM_MODEL = \"qwen2.5-coder:14b\"\n",
    "LLM_TEMPERATURE = 0.5\n",
    "LLM_TOP_P = 0.9\n",
    "DEFAULT_OLLAMA_PORT = \"11434\"\n",
    "TIMEOUT_SECONDS = 60 * 3\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# System Prompt for LLM\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert Python developer specializing in code refactoring, created to assist with optimizing Python scripts. Your role is to refactor Python code chunks—typically a single top-level function (def) or class—while preserving their original functionality.\n",
    "\n",
    "IMPORTANT: Your response must contain ONLY the refactored Python code with no explanations, comments about changes, or formatting markers. Do not include markdown code blocks, explanations, or anything else that is not part of the actual Python code.\n",
    "\"\"\"\n",
    "\n",
    "# User Prompt Template\n",
    "REFACTOR_PROMPT = \"\"\"\n",
    "Refactor this Python script chunk with the following improvements:\n",
    "\n",
    "1. Optimize performance without sacrificing readability—simplify logic, remove inefficiencies, and streamline operations.\n",
    "2. Remove unused imports and dead code (do not remove i18n references unless explicitly unused).\n",
    "3. Format according to Black's style (88-character line length, consistent indentation, sorted imports alphabetically).\n",
    "4. Enhance error handling with specific exceptions (e.g., ValueError, IOError) and actionable messages where missing.\n",
    "5. Replace comments with PEP 257-compliant docstrings for modules, functions, and classes; retain # for inline notes not suited to docstrings.\n",
    "6. Improve function and class names defined in this chunk to better reflect their purpose, using snake_case. Do not rename variables, functions, or classes that are not defined within this chunk (e.g., imported names).\n",
    "\n",
    "IMPORTANT: Respond with ONLY the refactored Python code, without explanations, markdown code blocks, or comments about the changes. Return just the code itself.\n",
    "\n",
    "Script chunk:\n",
    "{script_content}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def fetch_llm_response(\n",
    "    prompt: str,\n",
    "    system_prompt: str = SYSTEM_PROMPT,\n",
    "    model: str = LLM_MODEL,\n",
    "    temperature: float = LLM_TEMPERATURE,\n",
    "    top_p: float = LLM_TOP_P,\n",
    "    retries: int = MAX_RETRIES,\n",
    ") -> str:\n",
    "    \"\"\"Stream response from the LLM API and return the full refactored chunk.\"\"\"\n",
    "    if not prompt or not isinstance(prompt, str):\n",
    "        raise ValueError(\"Prompt must be a non-empty string\")\n",
    "\n",
    "    ollama_port = os.getenv(\"OLLAMA_PORT\", DEFAULT_OLLAMA_PORT)\n",
    "    url = f\"http://localhost:{ollama_port}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"system\": system_prompt,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": True,  # Enable streaming for live monitoring\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    full_response = \"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.post(url, json=payload, headers=headers, timeout=TIMEOUT_SECONDS, stream=True)\n",
    "            response.raise_for_status()\n",
    "            print(\"Streaming LLM response: \", end=\"\", flush=True)\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    chunk = line.decode(\"utf-8\")\n",
    "                    try:\n",
    "                        data = json.loads(chunk)\n",
    "                        if \"response\" in data:\n",
    "                            print(data[\"response\"], end=\"\", flush=True)\n",
    "                            full_response += data[\"response\"]\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"\\nWarning: Failed to parse chunk: {e}\", flush=True)\n",
    "            print()  # Newline after streaming completes\n",
    "\n",
    "            # Process the response to extract only the Python code\n",
    "            clean_response = extract_python_code(full_response)\n",
    "            return clean_response\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            if attempt == retries - 1:\n",
    "                raise requests.RequestException(f\"Failed after {retries} attempts: {e}\")\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}. Retrying in 2 seconds...\")\n",
    "            time.sleep(2)\n",
    "\n",
    "\n",
    "def extract_python_code(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract only the Python code from the LLM response, removing any explanations,\n",
    "    markdown code blocks, or other non-code content.\n",
    "    \"\"\"\n",
    "    # First, try to extract code from markdown code blocks if they exist\n",
    "    code_block_pattern = r\"```(?:python)?(.*?)```\"\n",
    "    code_blocks = re.findall(code_block_pattern, response, re.DOTALL)\n",
    "\n",
    "    if code_blocks:\n",
    "        # Join all code blocks (in case there are multiple)\n",
    "        return \"\\n\\n\".join(block.strip() for block in code_blocks)\n",
    "\n",
    "    # If no code blocks, try to identify response sections\n",
    "    explanation_markers = [\n",
    "        \"Here's the refactored\", \"The refactored\", \"I've refactored\",\n",
    "        \"Changes made:\", \"### Changes\", \"## Changes\", \"# Changes\",\n",
    "        \"Explanation:\", \"### Explanation\", \"## Explanation\", \"# Explanation\",\n",
    "        \"Summary of changes:\", \"In summary,\", \"To summarize,\"\n",
    "    ]\n",
    "\n",
    "    lines = response.split('\\n')\n",
    "    code_lines = []\n",
    "    in_code_section = True\n",
    "\n",
    "    for line in lines:\n",
    "        # Check if this line looks like the start of an explanation\n",
    "        if any(marker.lower() in line.lower() for marker in explanation_markers):\n",
    "            in_code_section = False\n",
    "            continue\n",
    "\n",
    "        if in_code_section:\n",
    "            code_lines.append(line)\n",
    "\n",
    "    # If we managed to filter out explanations, return the code\n",
    "    if code_lines:\n",
    "        return \"\\n\".join(code_lines)\n",
    "\n",
    "    # If all else fails, return the original response\n",
    "    # The validity check will catch if this isn't proper Python\n",
    "    return response\n",
    "\n",
    "\n",
    "def load_processed_files(tracker_file: str) -> Set[str]:\n",
    "    \"\"\"Load the set of already processed file paths from a tracker file.\"\"\"\n",
    "    processed = set()\n",
    "    if os.path.exists(tracker_file):\n",
    "        with open(tracker_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line and not line.startswith(\"#\"):\n",
    "                    processed.add(line)\n",
    "    return processed\n",
    "\n",
    "\n",
    "def save_processed_files(tracker_file: str, processed: Set[str]) -> None:\n",
    "    \"\"\"Save the set of processed file paths to a tracker file.\"\"\"\n",
    "    with open(tracker_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"# processed_files.txt\\n\")\n",
    "        f.write(\"# Tracks files already refactored\\n\\n\")\n",
    "        for file_path in sorted(processed):\n",
    "            f.write(f\"{file_path}\\n\")\n",
    "\n",
    "\n",
    "def split_into_top_level_chunks(script_content: str) -> List[Tuple[int, int, str]]:\n",
    "    \"\"\"\n",
    "    Split script into top-level 'def' or 'class' chunks with their start/end line numbers.\n",
    "\n",
    "    Returns a list of tuples containing (start_line, end_line, chunk_text).\n",
    "    \"\"\"\n",
    "    lines = script_content.splitlines()\n",
    "\n",
    "    # Find all top-level classes and function definitions\n",
    "    chunks = []\n",
    "    current_chunk_start = None\n",
    "    in_multiline_string = False\n",
    "    string_delimiter = None\n",
    "\n",
    "    # Get non-chunk content at the beginning of the file\n",
    "    non_chunk_lines = []\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        stripped = line.strip()\n",
    "\n",
    "        # Skip empty lines and comments at file beginning\n",
    "        if not stripped or stripped.startswith('#'):\n",
    "            if current_chunk_start is None:\n",
    "                non_chunk_lines.append(line)\n",
    "            continue\n",
    "\n",
    "        # Handle multiline strings\n",
    "        if not in_multiline_string:\n",
    "            # Check for start of multiline string\n",
    "            if ('\"\"\"' in line or \"'''\" in line):\n",
    "                # Determine which delimiter is used\n",
    "                if '\"\"\"' in line and \"'''\" in line:\n",
    "                    # Both appear, find the first one\n",
    "                    if line.find('\"\"\"') < line.find(\"'''\"):\n",
    "                        string_delimiter = '\"\"\"'\n",
    "                    else:\n",
    "                        string_delimiter = \"'''\"\n",
    "                elif '\"\"\"' in line:\n",
    "                    string_delimiter = '\"\"\"'\n",
    "                else:\n",
    "                    string_delimiter = \"'''\"\n",
    "\n",
    "                # Check if the multiline string is closed on the same line\n",
    "                if line.count(string_delimiter) % 2 != 0:\n",
    "                    in_multiline_string = True\n",
    "        else:\n",
    "            # Check for end of multiline string\n",
    "            if string_delimiter in line:\n",
    "                in_multiline_string = False\n",
    "            continue  # Skip processing this line further if in multiline string\n",
    "\n",
    "        # Detect the start of a new top-level chunk\n",
    "        if (line.startswith('def ') or line.startswith('class ')) and current_chunk_start is None:\n",
    "            # Store any non-chunk content at beginning of file\n",
    "            if non_chunk_lines and i > 0:\n",
    "                header_content = \"\\n\".join(non_chunk_lines)\n",
    "                chunks.append((0, i - 1, header_content))\n",
    "                non_chunk_lines = []\n",
    "\n",
    "            current_chunk_start = i\n",
    "        # Detect the end of the current chunk and the start of a new one\n",
    "        elif (line.startswith('def ') or line.startswith('class ')) and current_chunk_start is not None:\n",
    "            # Add the previous chunk\n",
    "            chunk_content = \"\\n\".join(lines[current_chunk_start:i])\n",
    "            chunks.append((current_chunk_start, i - 1, chunk_content))\n",
    "            current_chunk_start = i\n",
    "\n",
    "    # Add the final chunk if there is one\n",
    "    if current_chunk_start is not None:\n",
    "        chunk_content = \"\\n\".join(lines[current_chunk_start:])\n",
    "        chunks.append((current_chunk_start, len(lines) - 1, chunk_content))\n",
    "    # Or add any non-chunk content if the file has no chunks\n",
    "    elif non_chunk_lines:\n",
    "        header_content = \"\\n\".join(non_chunk_lines)\n",
    "        chunks.append((0, len(lines) - 1, header_content))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def is_valid_python(code: str) -> bool:\n",
    "    \"\"\"Check if the code is syntactically valid Python.\"\"\"\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        return True\n",
    "    except SyntaxError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def process_single_file(file_path: str) -> bool:\n",
    "    \"\"\"Process a single file by refactoring top-level 'def' or 'class' chunks one at a time.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            original_content = f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                raw = f.read()\n",
    "                detected = chardet.detect(raw)\n",
    "                encoding = detected[\"encoding\"] or \"latin1\"\n",
    "            with open(file_path, \"r\", encoding=encoding) as f:\n",
    "                original_content = f.read()\n",
    "            print(f\"Warning: {file_path} decoded with fallback encoding '{encoding}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error decoding {file_path}: {e}\")\n",
    "            return False\n",
    "    except OSError as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "    chunks = split_into_top_level_chunks(original_content)\n",
    "    if not chunks:\n",
    "        print(f\"Skipping {file_path}: No top-level 'def' or 'class' chunks found.\")\n",
    "        return False\n",
    "\n",
    "    print(f\"Found {len(chunks)} top-level chunks in {file_path}\")\n",
    "    modified_content = original_content\n",
    "\n",
    "    with tqdm(total=len(chunks), desc=f\"Refactoring {os.path.basename(file_path)}\", unit=\"chunk\") as pbar:\n",
    "        for i, (start_line, end_line, chunk_content) in enumerate(chunks, 1):\n",
    "            # Skip non-function/class chunks (like file headers, imports, etc.)\n",
    "            if not chunk_content.lstrip().startswith(('def ', 'class ')):\n",
    "                tqdm.write(f\"Skipping non-function/class chunk {i}/{len(chunks)} (lines {start_line + 1}-{end_line + 1})\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            tqdm.write(f\"Processing chunk {i}/{len(chunks)} (lines {start_line + 1}-{end_line + 1})\")\n",
    "            prompt = REFACTOR_PROMPT.format(script_content=chunk_content)\n",
    "\n",
    "            try:\n",
    "                refactored_chunk = fetch_llm_response(prompt)\n",
    "\n",
    "                # Debug output to understand what the refactored code looks like\n",
    "                tqdm.write(f\"Refactored chunk preview (first few lines):\")\n",
    "                preview_lines = refactored_chunk.split('\\n')[:5]\n",
    "                for line in preview_lines:\n",
    "                    tqdm.write(f\"  {line}\")\n",
    "                if len(preview_lines) < len(refactored_chunk.split('\\n')):\n",
    "                    tqdm.write(\"  ...\")\n",
    "\n",
    "                if not is_valid_python(refactored_chunk):\n",
    "                    tqdm.write(f\"Error: Refactored chunk {i} is invalid Python. Skipping this chunk.\")\n",
    "                    tqdm.write(f\"First 200 characters of invalid chunk: {refactored_chunk[:200]}\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "                # Replace the chunk in the content while preserving line numbers\n",
    "                original_lines = modified_content.splitlines()\n",
    "                refactored_lines = refactored_chunk.splitlines()\n",
    "\n",
    "                # Create new content by replacing the chunk\n",
    "                new_lines = original_lines[:start_line] + refactored_lines + original_lines[end_line + 1:]\n",
    "                new_content = \"\\n\".join(new_lines)\n",
    "\n",
    "                # Validate the combined code\n",
    "                if not is_valid_python(new_content):\n",
    "                    tqdm.write(f\"Error: Combined code after chunk {i} is invalid Python. Reverting this chunk.\")\n",
    "                    continue  # Keep the original content\n",
    "\n",
    "                # Update the modified content\n",
    "                modified_content = new_content\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                tqdm.write(f\"Error processing chunk {i}: {e}. Skipping this chunk.\")\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"Unexpected error processing chunk {i}: {e}. Skipping this chunk.\")\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Ensure trailing newline\n",
    "    if not modified_content.endswith(\"\\n\"):\n",
    "        modified_content += \"\\n\"\n",
    "\n",
    "    # Final validation\n",
    "    if not is_valid_python(modified_content):\n",
    "        print(f\"Error: Final refactored code for {file_path} is invalid Python. Keeping original.\")\n",
    "        return False\n",
    "\n",
    "    # Check if content actually changed\n",
    "    if modified_content == original_content:\n",
    "        print(f\"No changes were made to {file_path}\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(modified_content)\n",
    "        print(f\"Refactored and overwrote {file_path}\")\n",
    "        return True\n",
    "    except OSError as e:\n",
    "        print(f\"Error writing to {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_python_files(directory: str) -> List[str]:\n",
    "    \"\"\"Recursively find all Python files in the directory, excluding SKIP_FOLDERS.\"\"\"\n",
    "    python_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        dirs[:] = [d for d in dirs if d not in SKIP_FOLDERS]\n",
    "        for file in files:\n",
    "            if file.endswith(\".py\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                python_files.append(full_path)\n",
    "    return python_files\n",
    "\n",
    "\n",
    "def clear_terminal() -> None:\n",
    "    \"\"\"Clear the terminal screen.\"\"\"\n",
    "    os.system(\"cls\" if os.name == \"nt\" else \"clear\")\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"Process up to FILES_PER_RUN Python scripts with a progress bar.\"\"\"\n",
    "    if not os.path.exists(DEFAULT_DIRECTORY):\n",
    "        print(f\"Error: Directory {DEFAULT_DIRECTORY} does not exist.\")\n",
    "        return\n",
    "\n",
    "    processed_files = load_processed_files(PROCESSED_FILES_TRACKER)\n",
    "    python_files = get_python_files(DEFAULT_DIRECTORY)\n",
    "    if not python_files:\n",
    "        print(f\"No Python files found in {DEFAULT_DIRECTORY} (excluding {SKIP_FOLDERS}).\")\n",
    "        return\n",
    "\n",
    "    remaining_files = [f for f in python_files if f not in processed_files]\n",
    "    if not remaining_files:\n",
    "        print(\"All files have already been processed.\")\n",
    "        return\n",
    "\n",
    "    files_to_process = remaining_files[:FILES_PER_RUN]\n",
    "    print(f\"Found {len(remaining_files)} unprocessed files. Processing {len(files_to_process)} this run.\")\n",
    "    print(f\"Skipping folders: {SKIP_FOLDERS}\")\n",
    "\n",
    "    with tqdm(total=len(files_to_process), desc=\"Refactoring Files\", unit=\"file\") as pbar:\n",
    "        for i, file_path in enumerate(files_to_process, 1):\n",
    "            tqdm.write(f\"Processing file {i}/{len(files_to_process)}: {file_path}\")\n",
    "            if process_single_file(file_path):\n",
    "                processed_files.add(file_path)\n",
    "                save_processed_files(PROCESSED_FILES_TRACKER, processed_files)\n",
    "            pbar.update(1)\n",
    "            # Don't clear terminal to help with debugging\n",
    "            # clear_terminal()\n",
    "\n",
    "    remaining = len(remaining_files) - len(files_to_process)\n",
    "    print(f\"Processed {len(files_to_process)} files this run. {remaining} files remain unprocessed.\")\n",
    "    print(\"Refactoring batch complete! Check the results and run again for the next batch.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
