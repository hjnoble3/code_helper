{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Set, List, Optional, Dict, Any, Tuple\n",
    "import json\n",
    "\n",
    "PROCESSED_FILES_RECORD = \"processed_files.txt\"\n",
    "\n",
    "sources = {\n",
    "    \"OpenRouter\": {\n",
    "        \"base_url\": \"https://openrouter.ai/api/v1\",\n",
    "        \"model\": \"qwen/qwen-2.5-coder-32b-instruct:free\",\n",
    "        \"api_key\": \"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\"\n",
    "    },\n",
    "    \"Groq\": {\n",
    "        \"base_url\": \"https://api.groq.com/openai/v1/\",\n",
    "        \"model\": \"qwen-2.5-coder-32b\",\n",
    "        \"api_key\": \"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\"\n",
    "    },\n",
    "    \"Groq2\": {\n",
    "        \"base_url\": \"https://api.groq.com/openai/v1/\",\n",
    "        \"model\": \"qwen-2.5-coder-32b\",\n",
    "        \"api_key\": \"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\"\n",
    "    },\n",
    "    \"X.ai\": {\n",
    "        \"base_url\": \"https://api.x.ai/v1\",\n",
    "        \"model\": \"grok-2-latest\",\n",
    "        \"api_key\": \"xai-Lggu94vl22xIj5ThXFVu9nxbXjqYOqbTtLlxnibCypTkA5F8N4SvT0SDAYKIK8EkQCuVHv3tbfOPGiwt\"\n",
    "    },\n",
    "    \"Ollama\": {\n",
    "        \"base_url\": \"http://localhost:11434\",\n",
    "        \"model\": \"qwen2.5-coder\",\n",
    "        \"api_key\": None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Set the selected source\n",
    "source = \"Groq\"\n",
    "\n",
    "# Base configuration\n",
    "BASE_CONFIG = {\n",
    "    \"source_directories\": [r\"C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\components\\admin\"],\n",
    "    \"skip_directories\": [\n",
    "        r\"C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\",\n",
    "        r\"C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\i18n\",\n",
    "    ],\n",
    "    \"selected_pass\": \"combined\",\n",
    "    \"requests_per_minute\": 5,\n",
    "    \"concurrent_requests\": 3,\n",
    "    \"max_context_tokens\": 6000,\n",
    "    \"skip_long_files\": True,  # New option to skip files exceeding max_context_tokens\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.95,\n",
    "    \"file_extensions\": [\".py\", \".ts\", \".svelte\"],\n",
    "    \"retry_sleep_seconds\": 30,\n",
    "    \"prompt_templates\": {\n",
    "        \"optimize\": (\n",
    "            \"Optimize the {file_type} code while preserving core functionality:\\n\"\n",
    "            \"- Remove all i18n references (e.g., t(), $t, i18n.*) unless tied to core logic\\n\"\n",
    "            \"- Eliminate unused variables, imports, and dead code after verifying no external dependencies\\n\"\n",
    "            \"- Refactor repeated logic into reusable functions or components with descriptive names (e.g., calculateTotal, not calc)\\n\"\n",
    "            \"- Simplify code by:\\n\"\n",
    "            \"  - Using early returns to reduce nesting\\n\"\n",
    "            \"  - Replacing complex conditionals with clearer alternatives where possible\\n\"\n",
    "            \"- Improve performance by minimizing loops and redundant operations\\n\"\n",
    "            \"- Use {file_type}-specific best practices (e.g., async/await in .ts, reactive statements in .svelte)\\n\"\n",
    "            \"- Naming: variables/functions in camelCase, classes/components in PascalCase\\n\"\n",
    "            \"- Keep lines between 80-100 characters; break logically if longer\\n\"\n",
    "            \"- No empty lines between imports; one empty line before code\\n\"\n",
    "            \"- Preserve core logic, including onMount(() => {{) in .svelte files\\n\"\n",
    "            \"- Define core functionality as: primary operations the script performs, excluding optional features\\n\"\n",
    "            \"- Be decisive: apply the most effective optimization without alternatives\\n\"\n",
    "            \"Input code:\\n{content}\\n\\n\"\n",
    "            \"Return only the modified code without explanations or additional text.\"\n",
    "        ),\n",
    "        \"comment_cleanup\": (\n",
    "            \"Process the {file_type} code while preserving all functionality:\\n\"\n",
    "            \"- Remove all existing comments (e.g., //, #, /* */)\\n\"\n",
    "            \"- Add precise documentation as follows:\\n\"\n",
    "            \"  - For .py files: add a triple-quoted docstring ('''') to every function and top-level script block, including:\\n\"\n",
    "            \"    - Purpose: one sentence describing what it does\\n\"\n",
    "            \"    - Parameters: list each parameter with type and purpose\\n\"\n",
    "            \"    - Returns: describe the return value and type\\n\"\n",
    "            \"  - For .ts/.svelte files: add JSDoc comments (/** */) above every function and component, including:\\n\"\n",
    "            \"    - Purpose: one sentence describing what it does\\n\"\n",
    "            \"    - @param {{type}} name - purpose of each parameter\\n\"\n",
    "            \"    - @returns {{type}} - description of return value\\n\"\n",
    "            \"  - For HTML/Svelte markup: add <!-- Section: purpose --> comments to separate and describe major structural blocks (e.g., header, main content)\\n\"\n",
    "            \"- Keep comments concise: max 2 lines unless complex logic requires more\\n\"\n",
    "            \"- For complex logic, add brief inline comments (e.g., // Calculate total score)\\n\"\n",
    "            \"- No empty lines between imports; one empty line before code begins\\n\"\n",
    "            \"- Preserve core logic, including onMount(() => {{) in .svelte files\\n\"\n",
    "            \"Input code:\\n{content}\\n\\n\"\n",
    "            \"Return only the modified code without explanations or additional text.\"\n",
    "        ),\n",
    "        \"combined\": (\n",
    "            \"Combine the following two tasks for the {file_type} code while preserving core functionality:\\n\"\n",
    "            \"1. Optimize the code by removing all i18n references, unused variables, imports, dead code, and by refactoring repeated logic. \"\n",
    "            \"Apply general optimizations such as eliminating unnecessary if statements when only one option exists, \"\n",
    "            \"simplifying conditionals, and reducing redundant operations. \"\n",
    "            \"Use descriptive naming, leverage {file_type}-specific best practices, and maintain clear control flow. \"\n",
    "            \"Remove code relating to: ldap, updating version, oauth, haptic, mobile, channels, google drive, one drive.\\n\"\n",
    "            \"2. Clean up comments by removing all existing comments and adding clear, concise, and informative documentation. \"\n",
    "            \"Add comments to every function: for .py files, use docstrings with purpose, parameters, and returns; \"\n",
    "            \"for .ts/.svelte files, use JSDoc comments (purpose, @param, @returns). \"\n",
    "            \"For HTML/markup, add structured comments to describe the layout and purpose of every section and function.\\n\\n\"\n",
    "            \"Input code:\\n{content}\\n\\n\"\n",
    "            \"Return only the modified code without any explanations or additional text.\"\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "# Set up CONFIGS based on source\n",
    "if source == \"Groq\":\n",
    "    CONFIGS = [\n",
    "        {**BASE_CONFIG, \"base_url\": sources[\"Groq\"][\"base_url\"], \"api_key\": sources[\"Groq\"][\"api_key\"], \"model\": sources[\"Groq\"][\"model\"]},\n",
    "        {**BASE_CONFIG, \"base_url\": sources[\"Groq2\"][\"base_url\"], \"api_key\": sources[\"Groq2\"][\"api_key\"], \"model\": sources[\"Groq2\"][\"model\"]}\n",
    "    ]\n",
    "else:\n",
    "    CONFIGS = [{**BASE_CONFIG, \"base_url\": sources[source][\"base_url\"], \"api_key\": sources[source][\"api_key\"], \"model\": sources[source][\"model\"]}]\n",
    "\n",
    "\n",
    "def load_processed_files() -> Set[str]:\n",
    "    \"\"\"Load the list of already processed files.\"\"\"\n",
    "    if not os.path.exists(PROCESSED_FILES_RECORD):\n",
    "        return set()\n",
    "    with open(PROCESSED_FILES_RECORD, \"r\", encoding=\"utf-8\") as f:\n",
    "        return {line.strip() for line in f if line.strip()}\n",
    "\n",
    "\n",
    "def save_processed_files(file_paths: List[str]) -> None:\n",
    "    \"\"\"Add processed files to the record.\"\"\"\n",
    "    with open(PROCESSED_FILES_RECORD, \"a\", encoding=\"utf-8\") as f:\n",
    "        for fp in file_paths:\n",
    "            f.write(f\"{fp}\\n\")\n",
    "\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, requests_per_minute: int):\n",
    "        self.interval = 60.0 / requests_per_minute\n",
    "        self.last_request_time = 0\n",
    "        self.lock = asyncio.Lock()\n",
    "\n",
    "    async def acquire(self):\n",
    "        async with self.lock:\n",
    "            current_time = asyncio.get_event_loop().time()\n",
    "            elapsed = current_time - self.last_request_time\n",
    "            if elapsed < self.interval:\n",
    "                await asyncio.sleep(self.interval - elapsed)\n",
    "            self.last_request_time = asyncio.get_event_loop().time()\n",
    "\n",
    "\n",
    "async def refactor_code(session: aiohttp.ClientSession, file_path: str, prompt: str, rate_limiter: RateLimiter, sem: asyncio.Semaphore, config: Dict) -> str:\n",
    "    \"\"\"Call the API with a given prompt and return the modified code.\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {config['api_key']}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": config[\"model\"],\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": config[\"temperature\"],\n",
    "        \"top_p\": config[\"top_p\"],\n",
    "    }\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            await rate_limiter.acquire()\n",
    "            async with sem:\n",
    "                print(f\"Sending prompt to LLM for {file_path} using {config['api_key'][-4:]}\")\n",
    "                async with session.post(\n",
    "                    f\"{config['base_url']}/chat/completions\",\n",
    "                    json=payload,\n",
    "                    headers=headers,\n",
    "                    timeout=aiohttp.ClientTimeout(total=180)\n",
    "                ) as response:\n",
    "                    if response.status == 429:\n",
    "                        error_text = await response.text()\n",
    "                        error_json = json.loads(error_text)\n",
    "                        if error_json.get(\"error\", {}).get(\"type\") == \"tokens\":\n",
    "                            retry_after = error_json[\"error\"].get(\"retry_after\", config[\"retry_sleep_seconds\"])\n",
    "                            print(f\"Rate limit reached for {file_path}. Retrying after {retry_after} seconds.\")\n",
    "                            await asyncio.sleep(retry_after)\n",
    "                            continue\n",
    "                        else:\n",
    "                            print(f\"Rate limit error {response.status} for {file_path}: {error_text}\")\n",
    "                            return \"\"\n",
    "\n",
    "                    if response.status == 413:\n",
    "                        error_text = await response.text()\n",
    "                        print(f\"Request too large for {file_path}: {error_text}\")\n",
    "                        return \"\"\n",
    "\n",
    "                    if response.status != 200:\n",
    "                        error_text = await response.text()\n",
    "                        print(f\"API error {response.status} for {file_path}: {error_text}\")\n",
    "                        continue\n",
    "\n",
    "                    result = await response.json()\n",
    "                    if \"choices\" not in result or not result[\"choices\"]:\n",
    "                        print(f\"Missing 'choices' in API response for {file_path}: {result}\")\n",
    "                        continue\n",
    "\n",
    "                    code = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                    file_ext = Path(file_path).suffix[1:]\n",
    "                    markers = [f\"```{file_ext}\", \"```\"]\n",
    "                    if code.startswith(markers[0]) and code.endswith(markers[1]):\n",
    "                        code = code[len(markers[0]):-len(markers[1])].strip()\n",
    "                    return code or \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for {file_path}: {e}\")\n",
    "            if attempt < 2:\n",
    "                await asyncio.sleep(config[\"retry_sleep_seconds\"])\n",
    "    print(f\"All attempts failed for {file_path}\")\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "async def process_single_file(session: aiohttp.ClientSession, file_path: str, rate_limiter: RateLimiter, sem: asyncio.Semaphore, config: Dict) -> bool:\n",
    "    \"\"\"Process a single file using the selected pass prompt.\"\"\"\n",
    "    file_ext = Path(file_path).suffix[1:]\n",
    "    try:\n",
    "        async with aiofiles.open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            original_content = await f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "    estimated_tokens = len(original_content) // 4\n",
    "    if config[\"skip_long_files\"] and estimated_tokens > config[\"max_context_tokens\"]:\n",
    "        print(f\"Skipping {file_path} ({estimated_tokens} tokens) as it exceeds max_context_tokens ({config['max_context_tokens']})\")\n",
    "        return False\n",
    "\n",
    "    pass_name = config[\"selected_pass\"]\n",
    "    if pass_name not in config[\"prompt_templates\"]:\n",
    "        print(f\"Selected pass '{pass_name}' not found in prompt templates for {file_path}\")\n",
    "        return False\n",
    "\n",
    "    prompt = config[\"prompt_templates\"][pass_name].format(file_type=file_ext, content=original_content)\n",
    "    modified_content = await refactor_code(session, file_path, prompt, rate_limiter, sem, config)\n",
    "    if not modified_content:\n",
    "        print(f\"Failed to process {file_path}\")\n",
    "        return False\n",
    "\n",
    "    if modified_content != original_content:\n",
    "        try:\n",
    "            async with aiofiles.open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                await f.write(modified_content)\n",
    "            print(f\"Processed: {file_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to write {file_path}: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"No changes for: {file_path}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def should_skip_file(file_path: Path, skip_dirs: List[Path]) -> bool:\n",
    "    \"\"\"Check if a file should be skipped based on skip directories.\"\"\"\n",
    "    file_path_str = str(file_path.resolve())\n",
    "    for skip_dir in skip_dirs:\n",
    "        skip_dir_str = str(skip_dir)\n",
    "        if file_path_str.startswith(skip_dir_str) and (\n",
    "            len(file_path_str) == len(skip_dir_str) or file_path_str[len(skip_dir_str)] == os.path.sep\n",
    "        ):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def scan_directories() -> List[str]:\n",
    "    \"\"\"Scan source directories and return new file paths to process.\"\"\"\n",
    "    processed_files = load_processed_files()\n",
    "    file_paths = []\n",
    "    skip_dirs = [Path(d).resolve() for d in BASE_CONFIG[\"skip_directories\"]]\n",
    "\n",
    "    for directory in BASE_CONFIG[\"source_directories\"]:\n",
    "        dir_path = Path(directory).resolve()\n",
    "        if not dir_path.is_dir():\n",
    "            print(f\"Directory not found: {directory}\")\n",
    "            continue\n",
    "        for ext in BASE_CONFIG[\"file_extensions\"]:\n",
    "            for file_path in dir_path.glob(f\"**/*{ext}\"):\n",
    "                file_path = file_path.resolve()\n",
    "                if should_skip_file(file_path, skip_dirs) or str(file_path) in processed_files:\n",
    "                    continue\n",
    "                file_paths.append(str(file_path))\n",
    "    print(f\"Found {len(file_paths)} new files to process\")\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "def backup_files(file_paths: List[str]) -> str:\n",
    "    \"\"\"Backup files before processing.\"\"\"\n",
    "    if not file_paths:\n",
    "        print(\"No files to backup\")\n",
    "        return \"\"\n",
    "    backup_dir = f\"backup_{int(time.time())}\"\n",
    "    os.makedirs(backup_dir, exist_ok=True)\n",
    "    for fp in file_paths:\n",
    "        shutil.copy(fp, os.path.join(backup_dir, os.path.basename(fp)))\n",
    "    print(f\"Backed up {len(file_paths)} files to {backup_dir}\")\n",
    "    return backup_dir\n",
    "\n",
    "\n",
    "async def process_file_batch(session: aiohttp.ClientSession, file_paths: List[str], config: Dict, rate_limiter: RateLimiter, sem: asyncio.Semaphore) -> List[bool]:\n",
    "    \"\"\"Process a batch of files with a specific config.\"\"\"\n",
    "    tasks = [process_single_file(session, fp, rate_limiter, sem, config) for fp in file_paths]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "async def main(max_files_to_process: Optional[int] = None):\n",
    "    if not os.path.exists(PROCESSED_FILES_RECORD):\n",
    "        open(PROCESSED_FILES_RECORD, \"w\", encoding=\"utf-8\").close()\n",
    "\n",
    "    file_paths = scan_directories()\n",
    "    if max_files_to_process is not None:\n",
    "        file_paths = file_paths[:max_files_to_process]\n",
    "\n",
    "    if not file_paths:\n",
    "        print(\"No new files to process. Exiting.\")\n",
    "        return\n",
    "\n",
    "    backup_dir = backup_files(file_paths)\n",
    "\n",
    "    # Split files between endpoints if using Groq\n",
    "    if source == \"Groq\" and len(CONFIGS) == 2:\n",
    "        mid_point = len(file_paths) // 2\n",
    "        file_batches = [file_paths[:mid_point], file_paths[mid_point:]]\n",
    "    else:\n",
    "        file_batches = [file_paths]\n",
    "\n",
    "    rate_limiters = [RateLimiter(config[\"requests_per_minute\"]) for config in CONFIGS]\n",
    "    semaphores = [asyncio.Semaphore(config[\"concurrent_requests\"]) for config in CONFIGS]\n",
    "\n",
    "    connector = aiohttp.TCPConnector(limit=sum(config[\"concurrent_requests\"] for config in CONFIGS))\n",
    "    async with aiohttp.ClientSession(connector=connector) as session:\n",
    "        all_results = []\n",
    "        for config, files, rate_limiter, sem in zip(CONFIGS, file_batches, rate_limiters, semaphores):\n",
    "            if files:\n",
    "                print(f\"Processing {len(files)} files with API key ending in {config['api_key'][-4:]}\")\n",
    "                results = await process_file_batch(session, files, config, rate_limiter, sem)\n",
    "                all_results.extend(results)\n",
    "\n",
    "        processed_files = [fp for fp, success in zip(file_paths, all_results) if success]\n",
    "        save_processed_files(processed_files)\n",
    "        print(f\"Added {len(processed_files)} files to processed files record.\")\n",
    "\n",
    "    if backup_dir and os.path.exists(backup_dir):\n",
    "        shutil.rmtree(backup_dir)\n",
    "        print(f\"Removed backup directory {backup_dir}\")\n",
    "\n",
    "    print(f\"Processing complete. {len(processed_files)} files processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42 new files to process\n",
      "Backed up 16 files to backup_1742383631\n",
      "Processing 8 files with API key ending in Iwk5\n",
      "Sending prompt to LLM for C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\components\\admin\\Evaluations.svelte using Iwk5\n",
      "Processed: C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\components\\admin\\Evaluations.svelte\n",
      "Sending prompt to LLM for C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\components\\admin\\Functions.svelte using Iwk5\n",
      "Rate limit reached for C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\components\\admin\\Functions.svelte. Retrying after 30 seconds.\n",
      "Sending prompt to LLM for C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\components\\admin\\Evaluations\\FeedbackMenu.svelte using Iwk5\n",
      "Processed: C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\components\\admin\\Evaluations\\FeedbackMenu.svelte\n",
      "Sending prompt to LLM for C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\components\\admin\\Settings.svelte using Iwk5\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m main(max_files_to_process\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 343\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(max_files_to_process)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m files:\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m files with API key ending in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 343\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_file_batch(session, files, config, rate_limiter, sem)\n\u001b[0;32m    344\u001b[0m         all_results\u001b[38;5;241m.\u001b[39mextend(results)\n\u001b[0;32m    346\u001b[0m processed_files \u001b[38;5;241m=\u001b[39m [fp \u001b[38;5;28;01mfor\u001b[39;00m fp, success \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(file_paths, all_results) \u001b[38;5;28;01mif\u001b[39;00m success]\n",
      "Cell \u001b[1;32mIn[25], line 310\u001b[0m, in \u001b[0;36mprocess_file_batch\u001b[1;34m(session, file_paths, config, rate_limiter, sem)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Process a batch of files with a specific config.\"\"\"\u001b[39;00m\n\u001b[0;32m    309\u001b[0m tasks \u001b[38;5;241m=\u001b[39m [process_single_file(session, fp, rate_limiter, sem, config) \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m file_paths]\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "Cell \u001b[1;32mIn[25], line 242\u001b[0m, in \u001b[0;36mprocess_single_file\u001b[1;34m(session, file_path, rate_limiter, sem, config)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    241\u001b[0m prompt \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_templates\u001b[39m\u001b[38;5;124m\"\u001b[39m][pass_name]\u001b[38;5;241m.\u001b[39mformat(file_type\u001b[38;5;241m=\u001b[39mfile_ext, content\u001b[38;5;241m=\u001b[39moriginal_content)\n\u001b[1;32m--> 242\u001b[0m modified_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m refactor_code(session, file_path, prompt, rate_limiter, sem, config)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m modified_content:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to process \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 186\u001b[0m, in \u001b[0;36mrefactor_code\u001b[1;34m(session, file_path, prompt, rate_limiter, sem, config)\u001b[0m\n\u001b[0;32m    184\u001b[0m     retry_after \u001b[38;5;241m=\u001b[39m error_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry_after\u001b[39m\u001b[38;5;124m\"\u001b[39m, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretry_sleep_seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limit reached for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Retrying after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_after\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(retry_after)\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\asyncio\\tasks.py:665\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(delay, result)\u001b[0m\n\u001b[0;32m    661\u001b[0m h \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mcall_later(delay,\n\u001b[0;32m    662\u001b[0m                     futures\u001b[38;5;241m.\u001b[39m_set_result_unless_cancelled,\n\u001b[0;32m    663\u001b[0m                     future, result)\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    667\u001b[0m     h\u001b[38;5;241m.\u001b[39mcancel()\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "await main(max_files_to_process=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
