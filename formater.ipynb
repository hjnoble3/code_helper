{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Set, List, Optional, Dict\n",
    "import re\n",
    "\n",
    "PROCESSED_FILES_RECORD = \"processed_files.txt\"\n",
    "\n",
    "sources = {\n",
    "    \"OpenRouter\": {\n",
    "        \"base_url\": \"https://openrouter.ai/api/v1\",\n",
    "        \"model\": \"qwen/qwen-2.5-coder-32b-instruct:free\",\n",
    "        \"api_key\": \"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\"\n",
    "    },\n",
    "    \"Groq\": {\n",
    "        \"base_url\": \"https://api.groq.com/openai/v1/\",\n",
    "        \"model\": \"qwen-2.5-coder-32b\",\n",
    "        \"api_key\": \"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\"\n",
    "    },\n",
    "    \"Groq2\": {\n",
    "        \"base_url\": \"https://api.groq.com/openai/v1/\",\n",
    "        \"model\": \"qwen-2.5-coder-32b\",\n",
    "        \"api_key\": \"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\"\n",
    "    },\n",
    "    \"X.ai\": {\n",
    "        \"base_url\": \"https://api.x.ai/v1\",\n",
    "        \"model\": \"grok-2-latest\",\n",
    "        \"api_key\": \"xai-Lggu94vl22xIj5ThXFVu9nxbXjqYOqbTtLlxnibCypTkA5F8N4SvT0SDAYKIK8EkQCuVHv3tbfOPGiwt\"\n",
    "    },\n",
    "    \"Ollama\": {\n",
    "        \"base_url\": \"http://localhost:11434\",\n",
    "        \"model\": \"qwen2.5-coder\",\n",
    "        \"api_key\": None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Set the selected source\n",
    "source = \"X.ai\"\n",
    "\n",
    "# Base configuration\n",
    "BASE_CONFIG = {\n",
    "    \"source_directories\": [r\"C:\\Users\\harold.noble\\Desktop\\RIC\\app\\backend\"],\n",
    "    \"skip_directories\": [],\n",
    "    \"selected_pass\": \"python combined\",\n",
    "    \"requests_per_minute\": 5,\n",
    "    \"concurrent_requests\": 3,\n",
    "    \"max_context_tokens\": 5000,\n",
    "    \"skip_long_files\": False,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.95,\n",
    "    \"file_extensions\": [\".py\", \".ts\", \".svelte\"],\n",
    "    \"retry_sleep_seconds\": 30,\n",
    "    \"search_terms\": [],\n",
    "    \"prompt_templates\": {\n",
    "            \"python combined\": (\n",
    "                    \"\"\"\n",
    "                    Enhance Python Code with Documentation and Formatting\n",
    "                    Code Formatting and Preservation\n",
    "\n",
    "                        Preserve all functional code: Do not remove any working code, including variables, imports, or logic, unless explicitly unused (e.g., variables defined but never referenced).\n",
    "\n",
    "                        Reduce excessive blank lines: Remove multiple consecutive blank lines, leaving at most one blank line between code blocks for readability, per PEP 8.\n",
    "\n",
    "                        Refactor for clarity: Improve control flow using Python best practices (e.g., list comprehensions, context managers) only where it enhances readability, without altering behavior.\n",
    "\n",
    "                        Format: Adhere to PEP 8 style guidelines (e.g., 79-character line limit, proper indentation), consolidating single-line assignments where readability is not compromised.\n",
    "\n",
    "                        Maintain original structure: Keep the original intent, logic, and blank line spacing unless excessive (more than one consecutive blank line).\n",
    "\n",
    "                    Documentation Standards\n",
    "\n",
    "                        Docstrings:\n",
    "                            Add triple-quoted docstrings ('''Docstring''') to:\n",
    "                                Modules: At the file's start, describe its purpose and usage.\n",
    "                                Classes: Explain the class's purpose, key attributes, and usage.\n",
    "                                Functions: Detail the purpose, parameters, return values, and exceptions raised.\n",
    "                            Follow Google Python Style Guide format, keeping descriptions concise yet informative.\n",
    "                            Example:\n",
    "                            '''\n",
    "                            Short description of the function/class/module.\n",
    "                            Args:\n",
    "                                param1 (type): Description of param1.\n",
    "                                param2 (type): Description of param2.\n",
    "                            Returns:\n",
    "                                type: Description of the return value.\n",
    "                            Raises:\n",
    "                                ExceptionType: Conditions under which this is raised.\n",
    "                            '''\n",
    "\n",
    "                        High-Level Comments:\n",
    "                            Add single-line comments (# Purpose: Brief explanation) before major blocks (e.g., classes, functions, or complex logic) to describe their intent.\n",
    "                            Use clear, concise language focused on \"why\" rather than \"what.\"\n",
    "\n",
    "                        Inline Comments:\n",
    "                            Add inline comments (# Explanation) for non-obvious or complex logic, explaining the reasoning or necessity.\n",
    "                            Avoid redundant comments on self-explanatory code (e.g., no # Increment x for x += 1).\n",
    "                            Ensure comments enhance understanding without cluttering.\n",
    "\n",
    "                    Strict Guidelines\n",
    "\n",
    "                        Preserve original functionality: Do not alter the code's behavior in any way.\n",
    "                        Retain variable and function names: No renaming allowed.\n",
    "                        Do not delete blank lines unless they are excessive (more than one consecutive blank line).\n",
    "                        Return only the enhanced code: Include only the formatted code with added comments and docstrings, no extra text or summaries.\n",
    "                        Ensure every module, class, and function has a docstring.\n",
    "                        Follow PEP 8 (style) and PEP 257 (docstring conventions).\n",
    "\n",
    "                    Input: {content}\n",
    "                    \"\"\"\n",
    "                ),\n",
    "        \"combined\": (\n",
    "            \"\"\"\n",
    "            Optimize and refine the following {file_type} code while preserving its core functionality:\n",
    "\n",
    "            Code Optimization:\n",
    "            - Remove unused variables, imports, dead code, and redundant logic.\n",
    "            - Simplify conditionals by eliminating unnecessary if statements with a single outcome.\n",
    "            - Refactor repeated logic and enhance control flow using {file_type}-specific best practices.\n",
    "            - Exclude code related to version updates.\n",
    "            - Consolidate assignments and statements onto single lines where appropriate, minimizing unnecessary breaks.\n",
    "\n",
    "            Commenting & Documentation:\n",
    "            - Add concise, high-level comments above interface, functions, type, class or major code blocks.\n",
    "            - Include inline comments and property descriptions focusing on 'why' rather than 'what', unless the logic is complex.\n",
    "            - Use these comment formats:\n",
    "            - .py files: Triple-quoted docstrings (''') for functions and classes.\n",
    "            - .ts/.svelte files: JSDoc-style comments (/** */) above functions and interfaces. Keep a single line.\n",
    "            - HTML/Svelte markup: <!-- Purpose --> comments before major sections.\n",
    "\n",
    "            Do not alter functional behaviorâ€”focus solely on optimization and commenting.\n",
    "\n",
    "            Input code: {content}\n",
    "\n",
    "            Return only the optimized code with updated comments, without explanations or additional text.\n",
    "            \"\"\"\n",
    "        ),\n",
    "        \"adjust_comments\": (\n",
    "            \"Adjust comments in the {file_type} code while preserving all functionality:\\n\"\n",
    "            \"- Remove all existing comments\\n\"\n",
    "            \"- Add concise, meaningful comments as follows:\\n\"\n",
    "            \"  - For .py files:\\n\"\n",
    "            \"    - Add a triple-quoted docstring ('''') to every function, describing its purpose in 1-2 sentences\\n\"\n",
    "            \"      - Example: '''\\n      Initializes Pyodide and sets up global variables.\\n      '''\\n\"\n",
    "            \"      - Use multi-line docstrings with proper indentation and line breaks only if >80 characters\\n\"\n",
    "            \"      - Keep each line under 80 characters\\n\"\n",
    "            \"    - Add single-line comments (e.g., # Purpose) before significant code blocks\\n\"\n",
    "            \"  - For .ts/.svelte files:\\n\"\n",
    "            \"    - Add JSDoc-style comments above functions:\\n\"\n",
    "            \"      - Use single-line /** Purpose */ only if the description is simple, fits within 80 characters, and requires no tags\\n\"\n",
    "            \"      - Use multi-line /** */ format if the comment includes tags (@param, @returns, @throws) or exceeds 80 characters\\n\"\n",
    "            \"      - Example (single-line): /** Represents an Ollama model. */\\n\"\n",
    "            \"      - Example (multi-line): /**\\n       * Fetches audio configuration.\\n\"\n",
    "            \"       * @param string token - Bearer token for authentication.\\n\"\n",
    "            \"       * @returns object Audio configuration object.\\n\"\n",
    "            \"       * @throws Error Error if the request fails.\\n\"\n",
    "            \"       */\\n\"\n",
    "            \"    - Use single-line // Purpose for non-function code blocks or simple explanations\\n\"\n",
    "            \"  - For HTML/Svelte markup:\\n\"\n",
    "            \"    - Add single-line <!-- Purpose --> comments before major sections\\n\"\n",
    "            \"- Keep single-line comments (e.g., #, //, <!-- -->) and single-line JSDoc (/** */) under 80 characters\\n\"\n",
    "            \"- Use multi-line format for docstrings or JSDoc when including tags or if the description exceeds 80 characters\\n\"\n",
    "            \"- Focus comments on explaining 'why' rather than 'what', unless the logic is complex\\n\"\n",
    "            \"- Use single-line comments (e.g., //, #) instead of docstrings or JSDoc for trivial or self-explanatory functions\\n\"\n",
    "            \"- Do not modify any functional code, only adjust comments\\n\"\n",
    "            \"Input code:\\n{content}\\n\\n\"\n",
    "            \"Return only the modified code without explanations or additional text.\"\n",
    "        ),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Set up CONFIGS based on source\n",
    "if source == \"Groq\":\n",
    "    CONFIGS = [\n",
    "        {**BASE_CONFIG, \"base_url\": sources[\"Groq\"][\"base_url\"], \"api_key\": sources[\"Groq\"][\"api_key\"], \"model\": sources[\"Groq\"][\"model\"]},\n",
    "        {**BASE_CONFIG, \"base_url\": sources[\"Groq2\"][\"base_url\"], \"api_key\": sources[\"Groq2\"][\"api_key\"], \"model\": sources[\"Groq2\"][\"model\"]}\n",
    "    ]\n",
    "else:\n",
    "    CONFIGS = [{**BASE_CONFIG, \"base_url\": sources[source][\"base_url\"], \"api_key\": sources[source][\"api_key\"], \"model\": sources[source][\"model\"]}]\n",
    "\n",
    "\n",
    "def load_processed_files() -> Set[str]:\n",
    "    \"\"\"Load previously processed file paths.\"\"\"\n",
    "    return set(line.strip() for line in open(PROCESSED_FILES_RECORD, \"r\", encoding=\"utf-8\").readlines()) if os.path.exists(PROCESSED_FILES_RECORD) else set()\n",
    "\n",
    "def save_processed_files(file_paths: List[str]) -> None:\n",
    "    \"\"\"Append newly processed file paths to record.\"\"\"\n",
    "    with open(PROCESSED_FILES_RECORD, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(f\"{fp}\\n\" for fp in file_paths)\n",
    "\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, requests_per_minute: int):\n",
    "        self.interval = 60.0 / requests_per_minute\n",
    "        self.last_request_time = time.time()\n",
    "        self.lock = asyncio.Lock()\n",
    "\n",
    "    async def acquire(self):\n",
    "        async with self.lock:\n",
    "            current_time = time.time()\n",
    "            sleep_time = self.interval - (current_time - self.last_request_time)\n",
    "            if sleep_time > 0:\n",
    "                await asyncio.sleep(sleep_time)\n",
    "            self.last_request_time = time.time()\n",
    "\n",
    "\n",
    "async def refactor_code(session: aiohttp.ClientSession, file_path: str, prompt: str, rate_limiter: RateLimiter, sem: asyncio.Semaphore, config: Dict) -> str:\n",
    "    \"\"\"Refactor code using API with improved error handling.\"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {config['api_key']}\", \"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": config[\"model\"],\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": config[\"temperature\"],\n",
    "        \"top_p\": config[\"top_p\"],\n",
    "    }\n",
    "\n",
    "    async with sem:  # Only use semaphore as context manager\n",
    "        await rate_limiter.acquire()  # Call acquire directly\n",
    "        for attempt in range(3):\n",
    "            try:\n",
    "                async with session.post(\n",
    "                    f\"{config['base_url']}/chat/completions\",\n",
    "                    json=payload,\n",
    "                    headers=headers,\n",
    "                    timeout=aiohttp.ClientTimeout(total=180)\n",
    "                ) as response:\n",
    "                    if response.status != 200:\n",
    "                        if response.status == 429:\n",
    "                            await asyncio.sleep(config[\"retry_sleep_seconds\"])\n",
    "                            continue\n",
    "                        print(f\"API error {response.status} for {file_path}: {await response.text()}\")\n",
    "                        return \"\"\n",
    "\n",
    "                    result = await response.json()\n",
    "                    code = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "                    if not code:\n",
    "                        print(f\"Empty response for {file_path}\")\n",
    "                        return \"\"\n",
    "\n",
    "                    file_ext = Path(file_path).suffix[1:]\n",
    "                    markers = [f\"```{file_ext}\", \"```\"]\n",
    "                    if code.startswith(markers[0]) and code.endswith(markers[1]):\n",
    "                        code = code[len(markers[0]):-len(markers[1])].strip()\n",
    "                    return code\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Attempt {attempt + 1} failed for {file_path}: {e}\")\n",
    "                if attempt < 2:\n",
    "                    await asyncio.sleep(config[\"retry_sleep_seconds\"])\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def split_into_chunks(content: str, max_tokens: int) -> List[str]:\n",
    "    \"\"\"Split code into chunks based on token estimate (approx 4 chars per token).\"\"\"\n",
    "    lines = content.splitlines()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    token_limit = max_tokens * 4  # Rough estimate: 4 characters per token\n",
    "\n",
    "    for line in lines:\n",
    "        line_length = len(line)\n",
    "        if current_length + line_length > token_limit and current_chunk:\n",
    "            chunks.append(\"\\n\".join(current_chunk))\n",
    "            current_chunk = [line]\n",
    "            current_length = line_length\n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "            current_length += line_length + 1  # +1 for newline\n",
    "    if current_chunk:\n",
    "        chunks.append(\"\\n\".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "async def process_single_file(session: aiohttp.ClientSession, file_path: str, rate_limiter: RateLimiter, sem: asyncio.Semaphore, config: Dict) -> bool:\n",
    "    \"\"\"Process a single file, splitting into chunks if necessary.\"\"\"\n",
    "    file_ext = Path(file_path).suffix[1:]\n",
    "    try:\n",
    "        async with aiofiles.open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = await f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "    # Split into chunks if content is too long\n",
    "    chunks = split_into_chunks(content, config[\"max_context_tokens\"])\n",
    "    modified_chunks = []\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if config[\"skip_long_files\"] and (len(chunk) // 4) > config[\"max_context_tokens\"]:\n",
    "            print(f\"Skipping chunk {i+1} of {file_path} (too long)\")\n",
    "            modified_chunks.append(chunk)  # Keep original chunk if too long\n",
    "            continue\n",
    "\n",
    "        prompt = config[\"prompt_templates\"].get(config[\"selected_pass\"], \"\").format(file_type=file_ext, content=chunk)\n",
    "        if not prompt:\n",
    "            print(f\"Invalid pass for chunk {i+1} of {file_path}\")\n",
    "            modified_chunks.append(chunk)\n",
    "            continue\n",
    "\n",
    "        modified_content = await refactor_code(session, file_path, prompt, rate_limiter, sem, config)\n",
    "        if not modified_content:\n",
    "            modified_chunks.append(chunk)  # Keep original if refactoring fails\n",
    "        else:\n",
    "            modified_chunks.append(modified_content)\n",
    "\n",
    "    modified_content = \"\\n\".join(modified_chunks)\n",
    "    if modified_content == content:\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        async with aiofiles.open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            await f.write(modified_content)\n",
    "        print(f\"Processed: {file_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def should_skip_file(file_path: Path, skip_dirs: List[Path]) -> bool:\n",
    "    \"\"\"Check if file should be skipped based on directories.\"\"\"\n",
    "    file_path_str = str(file_path)\n",
    "    return any(file_path_str.startswith(str(skip_dir)) for skip_dir in skip_dirs)\n",
    "\n",
    "def scan_directories(config: Dict) -> List[str]:\n",
    "    \"\"\"Scan directories for files containing search terms.\"\"\"\n",
    "    processed_files = load_processed_files()\n",
    "    file_paths = []\n",
    "    skip_dirs = [Path(d).resolve() for d in config[\"skip_directories\"]]\n",
    "\n",
    "    for directory in config[\"source_directories\"]:\n",
    "        dir_path = Path(directory).resolve()\n",
    "        if not dir_path.is_dir():\n",
    "            continue\n",
    "        for ext in config[\"file_extensions\"]:\n",
    "            for file_path in dir_path.glob(f\"**/*{ext}\"):\n",
    "                file_path_str = str(file_path.resolve())\n",
    "                if file_path_str in processed_files or should_skip_file(file_path, skip_dirs):\n",
    "                    continue\n",
    "                file_paths.append(file_path_str)\n",
    "\n",
    "    print(f\"Found {len(file_paths)} potential files\")\n",
    "    return file_paths\n",
    "\n",
    "def backup_files(file_paths: List[str]) -> str:\n",
    "    \"\"\"Create backup of files.\"\"\"\n",
    "    if not file_paths:\n",
    "        return \"\"\n",
    "    backup_dir = f\"backup_{int(time.time())}\"\n",
    "    os.makedirs(backup_dir, exist_ok=True)\n",
    "    for fp in file_paths:\n",
    "        shutil.copy(fp, os.path.join(backup_dir, os.path.basename(fp)))\n",
    "    return backup_dir\n",
    "\n",
    "async def process_file_batch(session: aiohttp.ClientSession, file_paths: List[str], config: Dict, rate_limiter: RateLimiter, sem: asyncio.Semaphore) -> List[bool]:\n",
    "    \"\"\"Process a batch of files.\"\"\"\n",
    "    return await asyncio.gather(*[process_single_file(session, fp, rate_limiter, sem, config) for fp in file_paths])\n",
    "\n",
    "\n",
    "async def main(search_terms: List[str] = None, max_files_to_process: Optional[int] = None):\n",
    "    \"\"\"Main function with search term filtering and empty folder cleanup.\"\"\"\n",
    "    # Update config with search terms\n",
    "    for config in CONFIGS:\n",
    "        config[\"search_terms\"] = search_terms or []\n",
    "\n",
    "    if not os.path.exists(PROCESSED_FILES_RECORD):\n",
    "        open(PROCESSED_FILES_RECORD, \"w\", encoding=\"utf-8\").close()\n",
    "\n",
    "    # Scan all files first\n",
    "    all_file_paths = scan_directories(CONFIGS[0])\n",
    "\n",
    "    # Filter files containing search terms\n",
    "    matching_files = []\n",
    "    for file_path in all_file_paths:\n",
    "        try:\n",
    "            async with aiofiles.open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = await f.read()\n",
    "            pattern = '|'.join(rf\"(?:{re.escape(term)})\" for term in config[\"search_terms\"])\n",
    "            if re.search(pattern, content, re.IGNORECASE):\n",
    "                matching_files.append(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    print(f\"Found {len(matching_files)} files containing search terms: {search_terms}\")\n",
    "\n",
    "    # Apply max_files_to_process limit after filtering\n",
    "    file_paths = matching_files\n",
    "    if max_files_to_process is not None:\n",
    "        file_paths = file_paths[:max_files_to_process]\n",
    "\n",
    "    if not file_paths:\n",
    "        print(\"No matching files found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Processing {len(file_paths)} files\")\n",
    "    backup_dir = backup_files(file_paths)\n",
    "\n",
    "    file_batches = [file_paths] if source != \"Groq\" or len(CONFIGS) == 1 else [file_paths[:len(file_paths)//2], file_paths[len(file_paths)//2:]]\n",
    "\n",
    "    rate_limiters = [RateLimiter(config[\"requests_per_minute\"]) for config in CONFIGS]\n",
    "    semaphores = [asyncio.Semaphore(config[\"concurrent_requests\"]) for config in CONFIGS]\n",
    "\n",
    "    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=sum(c[\"concurrent_requests\"] for c in CONFIGS))) as session:\n",
    "        all_results = []\n",
    "        for config, files, rate_limiter, sem in zip(CONFIGS, file_batches, rate_limiters, semaphores):\n",
    "            if files:\n",
    "                results = await process_file_batch(session, files, config, rate_limiter, sem)\n",
    "                all_results.extend(results)\n",
    "\n",
    "        processed_files = [fp for fp, success in zip(file_paths, all_results) if success]\n",
    "        save_processed_files(processed_files)\n",
    "        print(f\"Processed {len(processed_files)} files\")\n",
    "\n",
    "    if backup_dir and os.path.exists(backup_dir):\n",
    "        shutil.rmtree(backup_dir)\n",
    "\n",
    "    # Clean up empty directories in source directories\n",
    "    for directory in CONFIGS[0][\"source_directories\"]:\n",
    "        dir_path = Path(directory).resolve()\n",
    "        if not dir_path.is_dir():\n",
    "            continue\n",
    "        for root, dirs, files in os.walk(dir_path, topdown=False):\n",
    "            for d in dirs:\n",
    "                dir_to_check = os.path.join(root, d)\n",
    "                try:\n",
    "                    if not os.listdir(dir_to_check):  # Check if directory is empty\n",
    "                        os.rmdir(dir_to_check)\n",
    "                        print(f\"Removed empty directory: {dir_to_check}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to remove {dir_to_check}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 potential files\n",
      "Found 1 files containing search terms: None\n",
      "Processing 1 files\n",
      "Processed: C:\\Users\\harold.noble\\Desktop\\RIC\\app\\backend\\webui_backend\\routers\\ollama.py\n",
      "Processed 1 files\n"
     ]
    }
   ],
   "source": [
    "await main(max_files_to_process=112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
