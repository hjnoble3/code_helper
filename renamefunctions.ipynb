{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"source_directories\": [r\"C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\"],\n",
    "    \"base_url\": \"https://api.groq.com/openai/v1/\",\n",
    "    \"model\": \"qwen-2.5-coder-32b\",\n",
    "    \"api_key\": \"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\",\n",
    "    \"file_extensions\": [\".py\", \".ts\", \".svelte\"],\n",
    "    \"max_context_tokens\": 1000,\n",
    "    \"requests_per_minute\": 5,\n",
    "    \"concurrent_requests\": 3,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.95,\n",
    "    \"retry_sleep_seconds\": 30,\n",
    "    \"prompt_template\": (\n",
    "        \"Analyze this {file_type} code and suggest better function names:\\n\"\n",
    "        \"1. Identify each function and its purpose.\\n\"\n",
    "        \"2. Propose new, descriptive names that reflect their functionality.\\n\"\n",
    "        \"3. Return a JSON list of objects with 'original_name' and 'new_name' fields.\\n\"\n",
    "        \"4. Do not modify the code itself, only return the name mappings.\\n\\n\"\n",
    "        \"Input code:\\n{content}\\n\\n\"\n",
    "        \"Return only the JSON list without additional explanation.\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Rate limiter to control the number of requests per minute.\"\"\"\n",
    "\n",
    "    def __init__(self, requests_per_minute: int):\n",
    "        self.interval = 60.0 / requests_per_minute\n",
    "        self.last_request_time = 0\n",
    "        self.lock = asyncio.Lock()\n",
    "\n",
    "    async def acquire(self):\n",
    "        async with self.lock:\n",
    "            current_time = asyncio.get_event_loop().time()\n",
    "            elapsed = current_time - self.last_request_time\n",
    "            if elapsed < self.interval:\n",
    "                await asyncio.sleep(self.interval - elapsed)\n",
    "            self.last_request_time = asyncio.get_event_loop().time()\n",
    "\n",
    "\n",
    "def split_into_chunks(content: str, max_tokens: int, file_type: str) -> List[str]:\n",
    "    \"\"\"Split content into chunks based on token estimate with improved boundary detection.\"\"\"\n",
    "    estimated_tokens = len(content) // 4\n",
    "    if estimated_tokens <= max_tokens:\n",
    "        return [content]\n",
    "\n",
    "    lines = content.splitlines()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_token_count = 0\n",
    "    boundary_markers = {\n",
    "        \"py\": [r\"def\\s+\\w+\\s*\\(\", r\"class\\s+\\w+\"],\n",
    "        \"ts\": [r\"function\\s+\\w+\\s*\\(\", r\"class\\s+\\w+\"],\n",
    "        \"svelte\": [r\"function\\s+\\w+\\s*\\(\", r\"<script>\"]\n",
    "    }.get(file_type, [r\"\\n\\s*\\n\"])\n",
    "\n",
    "    for line in lines:\n",
    "        line_tokens = len(line) // 4\n",
    "        if current_token_count + line_tokens > max_tokens and current_chunk:\n",
    "            chunk_text = \"\\n\".join(current_chunk)\n",
    "            last_boundary = -1\n",
    "            for marker in boundary_markers:\n",
    "                matches = list(re.finditer(marker, chunk_text))\n",
    "                if matches:\n",
    "                    last_boundary = max(last_boundary, matches[-1].start())\n",
    "            if last_boundary > 0:\n",
    "                split_point = last_boundary\n",
    "                chunks.append(chunk_text[:split_point].strip())\n",
    "                remaining = chunk_text[split_point:].strip()\n",
    "                current_chunk = [remaining] if remaining else []\n",
    "                current_token_count = len(remaining) // 4\n",
    "            else:\n",
    "                chunks.append(chunk_text.strip())\n",
    "                current_chunk = []\n",
    "                current_token_count = 0\n",
    "        current_chunk.append(line)\n",
    "        current_token_count += line_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\"\\n\".join(current_chunk).strip())\n",
    "\n",
    "    return [chunk for chunk in chunks if chunk]\n",
    "\n",
    "\n",
    "async def get_function_name_suggestions(session: aiohttp.ClientSession, file_path: str, content: str, file_type: str, rate_limiter: RateLimiter, sem: asyncio.Semaphore, config: Dict) -> List[Dict[str, str]]:\n",
    "    \"\"\"Send code to LLM and get function name suggestions with improved JSON parsing.\"\"\"\n",
    "    prompt = config[\"prompt_template\"].format(file_type=file_type, content=content)\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {config['api_key']}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": config[\"model\"],\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": config[\"temperature\"],\n",
    "        \"top_p\": config[\"top_p\"],\n",
    "    }\n",
    "\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            await rate_limiter.acquire()\n",
    "            async with sem:\n",
    "                print(f\"Sending prompt to LLM for {file_path}\")\n",
    "                async with session.post(\n",
    "                    f\"{config['base_url']}chat/completions\",\n",
    "                    json=payload,\n",
    "                    headers=headers,\n",
    "                    timeout=aiohttp.ClientTimeout(total=180)\n",
    "                ) as response:\n",
    "                    if response.status != 200:\n",
    "                        error_text = await response.text()\n",
    "                        print(f\"API error {response.status} for {file_path}: {error_text}\")\n",
    "                        continue\n",
    "\n",
    "                    result = await response.json()\n",
    "                    if \"choices\" not in result or not result[\"choices\"]:\n",
    "                        print(f\"Missing 'choices' in API response for {file_path}: {result}\")\n",
    "                        continue\n",
    "\n",
    "                    json_response = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                    # Clean up the response to handle potential formatting issues\n",
    "                    json_response = re.sub(r'^```json\\s*|\\s*```$', '', json_response, flags=re.MULTILINE).strip()\n",
    "                    try:\n",
    "                        mappings = json.loads(json_response)\n",
    "                        if not isinstance(mappings, list) or not all(isinstance(m, dict) and \"original_name\" in m and \"new_name\" in m for m in mappings):\n",
    "                            print(f\"Invalid response format for {file_path}: {json_response}\")\n",
    "                            return []\n",
    "                        return mappings\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Failed to parse JSON response for {file_path}: {json_response}\\nError: {e}\")\n",
    "                        return []\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed for {file_path}: {e}\")\n",
    "            if attempt < 2:\n",
    "                await asyncio.sleep(config[\"retry_sleep_seconds\"])\n",
    "    print(f\"All attempts failed for {file_path}\")\n",
    "    return []\n",
    "\n",
    "\n",
    "async def process_single_file(session: aiohttp.ClientSession, file_path: str, rate_limiter: RateLimiter, sem: asyncio.Semaphore, config: Dict) -> List[Dict[str, str]]:\n",
    "    \"\"\"Process a single file and return function name mappings.\"\"\"\n",
    "    file_ext = Path(file_path).suffix[1:]\n",
    "    all_mappings = []\n",
    "\n",
    "    try:\n",
    "        async with aiofiles.open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = await f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    estimated_tokens = len(content) // 4\n",
    "    if estimated_tokens > config[\"max_context_tokens\"]:\n",
    "        print(f\"File {file_path} ({estimated_tokens} tokens) exceeds max_context_tokens ({config['max_context_tokens']}). Splitting into chunks.\")\n",
    "        chunks = split_into_chunks(content, config[\"max_context_tokens\"], file_ext)\n",
    "    else:\n",
    "        chunks = [content]\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        mappings = await get_function_name_suggestions(session, f\"{file_path} (chunk {i+1}/{len(chunks)})\", chunk, file_ext, rate_limiter, sem, config)\n",
    "        if mappings:\n",
    "            all_mappings.extend(mappings)\n",
    "        else:\n",
    "            print(f\"Failed to get suggestions for chunk {i+1} of {file_path}\")\n",
    "\n",
    "    return all_mappings\n",
    "\n",
    "\n",
    "async def scan_directories(config: Dict) -> List[str]:\n",
    "    \"\"\"Scan source directories and return file paths.\"\"\"\n",
    "    file_paths = []\n",
    "    for directory in config[\"source_directories\"]:\n",
    "        dir_path = Path(directory).resolve()\n",
    "        if not dir_path.is_dir():\n",
    "            print(f\"Directory not found: {directory}\")\n",
    "            continue\n",
    "        for ext in config[\"file_extensions\"]:\n",
    "            for file_path in dir_path.glob(f\"**/*{ext}\"):\n",
    "                file_paths.append(str(file_path.resolve()))\n",
    "    print(f\"Found {len(file_paths)} files to process\")\n",
    "    return file_paths\n",
    "\n",
    "\n",
    "async def main(max_files_to_process: Optional[int] = None):\n",
    "    file_paths = await scan_directories(CONFIG)\n",
    "    if max_files_to_process is not None:\n",
    "        file_paths = file_paths[:max_files_to_process]\n",
    "\n",
    "    if not file_paths:\n",
    "        print(\"No files to process. Exiting.\")\n",
    "        return\n",
    "\n",
    "    rate_limiter = RateLimiter(CONFIG[\"requests_per_minute\"])\n",
    "    semaphore = asyncio.Semaphore(CONFIG[\"concurrent_requests\"])\n",
    "    all_mappings = {}\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [process_single_file(session, fp, rate_limiter, semaphore, CONFIG) for fp in file_paths]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        for file_path, mappings in zip(file_paths, results):\n",
    "            if mappings:\n",
    "                all_mappings[file_path] = mappings\n",
    "\n",
    "    print(f\"Processing complete. Analyzed {len(all_mappings)} files.\")\n",
    "    if all_mappings:\n",
    "        print(\"\\nFunction name suggestions:\")\n",
    "        for file_path, mappings in all_mappings.items():\n",
    "            print(f\"\\n{file_path}:\")\n",
    "            for mapping in mappings:\n",
    "                print(f\"  {mapping['original_name']} -> {mapping['new_name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 files to process\n",
      "File C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\\auths\\index.ts (1670 tokens) exceeds max_context_tokens (1000). Splitting into chunks.\n",
      "Sending prompt to LLM for C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\\auths\\index.ts (chunk 1/2)\n",
      "File C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\\configs\\index.ts (1495 tokens) exceeds max_context_tokens (1000). Splitting into chunks.\n",
      "File C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\\index.ts (5060 tokens) exceeds max_context_tokens (1000). Splitting into chunks.\n",
      "File C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\\chats\\index.ts (4634 tokens) exceeds max_context_tokens (1000). Splitting into chunks.\n",
      "Sending prompt to LLM for C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\\audio\\index.ts (chunk 1/1)\n",
      "Sending prompt to LLM for C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\\configs\\index.ts (chunk 1/2)\n",
      "Sending prompt to LLM for C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\\index.ts (chunk 1/5)\n",
      "Sending prompt to LLM for C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\\chats\\index.ts (chunk 1/5)\n",
      "Sending prompt to LLM for C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\\auths\\index.ts (chunk 2/2)\n",
      "Sending prompt to LLM for C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\\configs\\index.ts (chunk 2/2)\n",
      "Sending prompt to LLM for C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\\index.ts (chunk 2/5)\n",
      "Sending prompt to LLM for C:\\Users\\harold.noble\\Desktop\\RIC\\app\\frontend\\src\\lib\\apis\\chats\\index.ts (chunk 2/5)\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m main(max_files_to_process\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 209\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(max_files_to_process)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m    208\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [process_single_file(session, fp, rate_limiter, semaphore, CONFIG) \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m file_paths]\n\u001b[1;32m--> 209\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file_path, mappings \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(file_paths, results):\n\u001b[0;32m    211\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mappings:\n",
      "Cell \u001b[1;32mIn[3], line 170\u001b[0m, in \u001b[0;36mprocess_single_file\u001b[1;34m(session, file_path, rate_limiter, sem, config)\u001b[0m\n\u001b[0;32m    167\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m [content]\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[1;32m--> 170\u001b[0m     mappings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_function_name_suggestions(session, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m, chunk, file_ext, rate_limiter, sem, config)\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mappings:\n\u001b[0;32m    172\u001b[0m         all_mappings\u001b[38;5;241m.\u001b[39mextend(mappings)\n",
      "Cell \u001b[1;32mIn[3], line 111\u001b[0m, in \u001b[0;36mget_function_name_suggestions\u001b[1;34m(session, file_path, content, file_type, rate_limiter, sem, config)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m rate_limiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m sem:\n\u001b[0;32m    113\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending prompt to LLM for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 47\u001b[0m, in \u001b[0;36mRateLimiter.acquire\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m current_time \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_request_time\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m elapsed \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterval:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterval \u001b[38;5;241m-\u001b[39m elapsed)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_request_time \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\asyncio\\tasks.py:665\u001b[0m, in \u001b[0;36msleep\u001b[1;34m(delay, result)\u001b[0m\n\u001b[0;32m    661\u001b[0m h \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mcall_later(delay,\n\u001b[0;32m    662\u001b[0m                     futures\u001b[38;5;241m.\u001b[39m_set_result_unless_cancelled,\n\u001b[0;32m    663\u001b[0m                     future, result)\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 665\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    667\u001b[0m     h\u001b[38;5;241m.\u001b[39mcancel()\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "await main(max_files_to_process=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
